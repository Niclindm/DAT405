{"cells":[{"cell_type":"markdown","source":"# DAT405 Introduction to Data Science and AI \n## 2022-2023, Reading Period 3\n## Assignment 5: Reinforcement learning and classification -- Group 85\n\n**Authors: Niclas Lindmark, Anton Johansson, Noa Sjöstrand**\n\n**Date: 21/2 - 2023**\n\nThe exercise takes place in a notebook environment where you can chose to use Jupyter or Google Colabs. We recommend you use Google Colabs as it will facilitate remote group-work and makes the assignment less technical. \nHints:\nYou can execute certain linux shell commands by prefixing the command with `!`. You can insert Markdown cells and code cells. The first you can use for documenting and explaining your results the second you can use writing code snippets that execute the tasks required.  \n\nThis assignment is about **sequential decision making** under uncertainty (Reinforcement learning). In a sequential decision process, the process jumps between different states (the environment), and in each state the decision maker, or agent, chooses among a set of actions. Given the state and the chosen action, the process jumps to a new state. At each jump the decision maker receives a reward, and the objective is to find a sequence of decisions (or an optimal policy) that maximizes the accumulated rewards.\n\nWe will use **Markov decision processes** (MDPs) to model the environment, and below is a primer on the relevant background theory. \n","metadata":{"id":"R3D82waLqItO","cell_id":"c6266f52d48c4844859956ebe67ce71c","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":1},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"\n* To make things concrete, we will first focus on decision making under **no** uncertainity (question 1 and 2), i.e, given we have a world model, we can calculate the exact and optimal actions to take in it. We will first introduce **Markov Decision Process (MDP)** as the world model. Then we give one algorithm (out of many) to solve it.\n\n* (Optional) Next we will work through one type of reinforcement learning algorithm called Q-learning (question 3). Q-learning is an algorithm for making decisions under uncertainity, where uncertainity is over the possible world model (here MDP). It will find the optimal policy for the **unknown** MDP, assuming we do infinite exploration.\n\n* Finally, in question 4 you will be asked to explain differences between reinforcement learning and supervised learning and in question 5 write about decision trees and random forests.","metadata":{"id":"8jEcC9NKqItQ","cell_id":"f371412c5b9d4aef8bef7481ef1d60f3","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":7},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Primer\n### Decision Making\nThe problem of **decision making under uncertainty** (commonly known as **reinforcement learning**) can be broken down into\ntwo parts. First, how do we learn about the world? This involves both the\nproblem of modeling our initial uncertainty about the world, and that of drawing conclusions from evidence and our initial belief. Secondly, given what we\ncurrently know about the world, how should we decide what to do, taking into\naccount future events and observations that may change our conclusions?\nTypically, this will involve creating long-term plans covering possible future\neventualities. That is, when planning under uncertainty, we also need to take\ninto account what possible future knowledge could be generated when implementing our plans. Intuitively, executing plans which involve trying out new\nthings should give more information, but it is hard to tell whether this information will be beneficial. The choice between doing something which is already\nknown to produce good results and experiment with something new is known\nas the **exploration-exploitation dilemma**.\n\n### The exploration-exploitation trade-off\n\nConsider the problem of selecting a restaurant to go to during a vacation. Lets say the\nbest restaurant you have found so far was **Les Epinards**. The food there is\nusually to your taste and satisfactory. However, a well-known recommendations\nwebsite suggests that **King’s Arm** is really good! It is tempting to try it out. But\nthere is a risk involved. It may turn out to be much worse than **Les Epinards**,\nin which case you will regret going there. On the other hand, it could also be\nmuch better. What should you do?\nIt all depends on how much information you have about either restaurant,\nand how many more days you’ll stay in town. If this is your last day, then it’s\nprobably a better idea to go to **Les Epinards**, unless you are expecting **King’s\nArm** to be significantly better. However, if you are going to stay there longer,\ntrying out **King’s Arm** is a good bet. If you are lucky, you will be getting much\nbetter food for the remaining time, while otherwise you will have missed only\none good meal out of many, making the potential risk quite small.","metadata":{"id":"uGtknnUVqItP","cell_id":"75ed34d1b8f34e329922310262bb98cd","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":13},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"### Markov Decision Processes\nMarkov Decision Processes (MDPs) provide a mathematical framework for modeling sequential decision making under uncertainty. An *agent* moves between *states* in a *state space* choosing *actions* that affects the transition probabilities between states, and the subsequent *rewards* recieved after a jump. This is then repeated a finite or infinite number of epochs. The objective, or the *solution* of the MDP, is to optimize the accumulated rewards of the process.\n\nThus, an MDP consists of five parts: \n\n* Decision epochs: $t={1,2,...,T}$, where $T\\leq \\infty$\n* State space: $S=\\{s_1,s_2,...,s_N\\}$ of the underlying environment\n* Action space $A=\\{a_1,a_2,...,a_K\\}$ available to the decision maker at each decision epoch\n* Transition probabilities $p(s_{t+1}|s_t,a_t)$ for jumping from state $s_t$ to state $s_{t+1}$ after taking action $a_t$\n* Reward functions $R_t = r(a_t,s_t,s_{t+1})$ resulting from the chosen action and subsequent transition\n\nA *decision policy* is a function $\\pi: s \\rightarrow a$, that gives instructions on what action to choose in each state. A policy can either be *deterministic*, meaning that the action is given for each state, or *randomized* meaning that there is a probability distribution over the set of possible actions for each state. Given a specific policy $\\pi$ we can then compute the the *expected total reward* when starting in a given state $s_1 \\in S$, which is also known as the *value* for that state, \n\n$$V^\\pi (s_1) = E\\left[ \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) {\\Large |} s_1\\right] = \\sum_{t=1}^{T} r(s_t,a_t,s_{t+1}) p(s_{t+1} | a_t,s_t)$$ \n\nwhere $a_t = \\pi(s_t)$. To ensure convergence and to control how much credit to give to future rewards, it is common to introduce a *discount factor* $\\gamma \\in [0,1]$. For instance, if we think all future rewards should count equally, we would use $\\gamma = 1$, while if we value near-future rewards higher than more distant rewards, we would use $\\gamma < 1$. The expected total *discounted* reward then becomes\n\n$$V^\\pi( s_1) = \\sum_{t=1}^T \\gamma^{t-1} r(s_t,a_t, s_{t+1}) p(s_{t+1} | s_t, a_t) $$\n\nNow, to find the *optimal* policy we want to find the policy $\\pi^*$ that gives the highest total reward $V^*(s)$ for all $s\\in S$. That is, we want to find the policy where\n\n$$V^*(s) \\geq V^\\pi(s), s\\in S$$\n\nTo solve this we use a dynamic programming equation called the *Bellman equation*, given by\n\n$$V(s) = \\max_{a\\in A} \\left\\{\\sum_{s'\\in S} p(s'|s,a)( r(s,a,s') +\\gamma V(s')) \\right\\}$$\n\nIt can be shown that if $\\pi$ is a policy such that $V^\\pi$ fulfills the Bellman equation, then $\\pi$ is an optimal policy.\n\nA real world example would be an inventory control system. The states could be the amount of items we have in stock, and the actions would be the amount of items to order at the end of each month. The discrete time would be each month and the reward would be the profit. \n","metadata":{"id":"h9WIePUCqItR","cell_id":"eb3deb57cc58481e84ca9fcca952551c","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":19},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Question 1","metadata":{"id":"KiO_zpY7qItS","cell_id":"a15cf0df206c4ec8844200f488e5ab11","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":25},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"The first question covers a deterministic MPD, where the action is directly given by the state, described as follows:\n\n* The agent starts in state **S** (see table below)\n* The actions possible are **N** (north), **S** (south), **E** (east), and **W** west. \n* The transition probabilities in each box are deterministic (for example P(s'|s,N)=1 if s' north of s). Note, however, that you cannot move outside the grid, thus all actions are not available in every box.\n* When reaching **F**, the game ends (absorbing state).\n* The numbers in the boxes represent the rewards you receive when moving into that box. \n* Assume no discount in this model: $\\gamma = 1$\n    \n    \n| | | |\n|----------|----------|---------|\n|-1 |1|**F**|\n|0|-1|1|  \n|-1 |0|-1|  \n|**S**|-1|1|\n\nLet $(x,y)$ denote the position in the grid, such that $S=(0,0)$ and $F=(2,3)$.\n\n**1a)** What is the optimal path of the MDP above? Is it unique? Submit the path as a single string of directions. E.g. NESW will make a circle.\n\n**1b)** What is the optimal policy (i.e. the optimal action in each state)? It is helpful if you draw the arrows/letters in the grid.\n\n**1c)** What is expected total reward for the policy in 1a)?\n","metadata":{"id":"XUyGq4olqItS","cell_id":"17b795838f9743f6b5e556aa4acdc153","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":31},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Answer:","metadata":{"tags":[],"cell_id":"3062d52e67d543748e4954d3393f18f5","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":7,"fromCodePoint":0}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":148},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"a) The optimal path is **EENNN**. However this path is not unique as it is also possible to reach the same result by taking a longer route, such as EENNWNE, or by moving back and forth between states during the process. In either case, the total reward will remain unchanged.\n\nb) Policy is visualized in the attached picture below. The red arrows indicate a deterministic policy and the blue arrows indicate a stocastic policy. The agent can move in the direction of either of the blue arrows. Also we assume that the agent can see the whole state space, and evaluate the reward of both current and future actions. Therefore the next action is based on the accumilated reward from current state to F. \n\nc) 0","metadata":{"cell_id":"edb29073ec5c4a128eac83c6d31ec067","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":37},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"![Picture title](image-20230221-105649.png)","metadata":{"tags":[],"cell_id":"c733f8302f9848bf815c0398a7840115","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":43},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Value Iteration","metadata":{"id":"sNkIk-k7qItT","cell_id":"667d6038e2504530b7ad1809fb4f544b","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":49},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"For larger problems we need to utilize algorithms to determine the optimal policy $\\pi^*$. *Value iteration* is one such algorithm that iteratively computes the value for each state. Recall that for a policy to be optimal, it must satisfy the Bellman equation above, meaning that plugging in a given candidate $V^*$ in the right-hand side (RHS) of the Bellman equation should result in the same $V^*$ on the left-hand side (LHS). This property will form the basis of our algorithm. Essentially, it can be shown that repeated application of the RHS to any intial value function $V^0(s)$ will eventually lead to the value $V$ which statifies the Bellman equation. Hence repeated application of the Bellman equation will also lead to the optimal value function. We can then extract the optimal policy by simply noting what actions that satisfy the equation.    ","metadata":{"id":"NJTFDikEqItT","cell_id":"432099c5fba146a3b63f31d0f21131fb","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":55},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"The process of repeated application of the Bellman equation is what we here call the _value iteration_ algorithm. It practically procedes as follows:\n\n```\nepsilon is a small value, threshold\nfor x from i to infinity \ndo\n    for each state s\n    do\n        V_k[s] = max_a Σ_s' p(s′|s,a)*(r(a,s,s′) + γ*V_k−1[s′])\n    end\n    if  |V_k[s]-V_k-1[s]| < epsilon for all s\n        for each state s,\n        do\n            π(s)=argmax_a ∑_s′ p(s′|s,a)*(r(a,s,s′) + γ*V_k−1[s′])\n            return π, V_k \n        end\nend\n\n```","metadata":{"id":"3ZdhW0AZDoZv","cell_id":"d4511cf7bf9544689cdbc5b882dd1274","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":61},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"**Example:** We will illustrate the value iteration algorithm by going through two iterations. Below is a 3x3 grid with the rewards given in each state. Assume now that given a certain state $s$ and action $a$, there is a probability 0.8 that that action will be performed and a probability 0.2 that no action is taken. For instance, if we take action **E** in state $(x,y)$ we will go to $(x+1,y)$ 80 percent of the time (given that that action is available in that state), and remain still 20 percent of the time. We will use have a discount factor $\\gamma = 0.9$. Let the initial value be $V^0(s)=0$ for all states $s\\in S$. \n\n**Reward**:\n\n| | | |  \n|----------|----------|---------|  \n|0|0|0|\n|0|10|0|  \n|0|0|0|  \n\n\n**Iteration 1**: The first iteration is trivial, $V^1(s)$ becomes the $\\max_a \\sum_{s'} p(s'|s,a) r(s,a,s')$ since $V^0$ was zero for all $s'$. The updated values for each state become\n\n| | | |  \n|----------|----------|---------|  \n|0|8|0|\n|8|2|8|  \n|0|8|0|  \n  \n**Iteration 2**:  \n  \nStaring with cell (0,0) (lower left corner): We find the expected value of each move:  \nAction **S**: 0  \nAction **E**: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \nAction **N**: 0.8( 0 + 0.9 \\* 8) + 0.2(0 + 0.9 \\* 0) = 5.76  \nAction **W**: 0\n\nHence any action between **E** and **N** would be best at this stage.\n\nSimilarly for cell (1,0):\n\nAction **N**: 0.8( 10 + 0.9 \\* 2) + 0.2(0 + 0.9 \\* 8) = 10.88 (Action **N** is the maximizing action)  \n\nSimilar calculations for remaining cells give us:\n\n| | | |  \n|----------|----------|---------|  \n|5.76|10.88|5.76|\n|10.88|8.12|10.88|  \n|5.76|10.88|5.76|  \n","metadata":{"id":"Nz3UqgozqItU","cell_id":"da9305c687d842baa0db612f55914718","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":67},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Question 2\n\n**2a)** Code the value iteration algorithm just described here, and show the converging optimal value function and the optimal policy for the above 3x3 grid.\n\n**2b)** Explain why the result of 2a) does not depend on the initial value $V_0$.\n\n**2c)** Describe your interpretation of the discount factor $\\gamma$. What would happen in the two extreme cases $\\gamma = 0$ and $\\gamma = 1$? Given some MDP, what would be important things to consider when deciding on which value of $\\gamma$ to use?","metadata":{"id":"S3vIdFpuqItU","cell_id":"dff8db10b10448c79e9c44b7edd80075","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":73},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Answers:","metadata":{"tags":[],"cell_id":"5649f84011764bc6b67b85f13324be92","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":8,"fromCodePoint":0}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":136},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"2a)","metadata":{"tags":[],"cell_id":"438f1f7e-5540-4db1-8c2d-025b2b280334","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":139},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"code","source":"import numpy as np\n\n\n# Define the reward matrix\nreward_matrix = np.array(([0,0,0],\n                          [0,10,0],\n                          [0,0,0])).astype(float)\n\n# Define the initial state matrix\nstate_matrix = np.array(([0,0,0],\n                         [0,0,0],\n                         [0,0,0])).astype(float)\n\n# Create a comparison matrix to check for convergence later\ncomparison_matrix = state_matrix\n\n# Set the probability of a successful move and the available actions\nprobability_move = 0.8\nactions = [[0,1], [0,-1], [-1,0], [1,0]]\n\n# Set the convergence threshold and discount factor\nepsilon = 0.01\ngamma = 0.9\n\n# Initialize variables\nchange = 1\na = 0\n\n# Loop until convergence\nwhile True:\n    # Make a copy of the current state matrix to check for convergence later\n    previous_matrix = state_matrix.copy()\n\n    # count the number of iterations\n    a += 1\n    \n    # Iterate over each state in the matrix\n    for i in range(len(reward_matrix)):\n        for j in range(len(reward_matrix[i])):\n            # Initialize the maximum reward for this state\n            max_reward = 0\n            \n            # Iterate over each action\n            for action in actions:\n                # Check if the action is valid (i.e., it won't move the agent off the grid)\n                if action[0] == 1 and i == (len(reward_matrix)-1):\n                    continue\n                if action[0] == -1 and i == 0:\n                    continue\n                if action[1] == 1 and j == (len(reward_matrix[i])-1):\n                    continue\n                if action[1] == -1 and j == 0:\n                    continue\n                \n                # Calculate the reward for taking this action in this state\n                reward = probability_move*(reward_matrix[i+action[0]][j+action[1]]+gamma*state_matrix[i+action[0]][j+action[1]]) + (1-probability_move)*(reward_matrix[i][j]+gamma*state_matrix[i][j])\n                \n                # If this reward is higher than the previous maximum, update the maximum\n                if reward > max_reward:\n                    max_reward = reward\n            \n            # Update the state value for this state with the maximum reward\n            state_matrix[i][j] = max_reward\n    \n    # Check if the state matrix has converged\n    if abs(state_matrix-previous_matrix).sum() < epsilon:\n        break\n\n# Print the resulting state matrix\nprint(state_matrix.round(2),f\"\\n iterations: {a}\")","metadata":{"tags":[],"cell_id":"801f72cb73294f74b3380fdd72ddd47b","source_hash":"7e34cf1c","execution_start":1676991285817,"execution_millis":33,"deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":121},"deepnote_to_be_reexecuted":false,"deepnote_cell_type":"code"},"outputs":[{"name":"stdout","text":"[[45.61 51.94 45.61]\n [51.94 48.05 51.94]\n [45.61 51.94 45.61]] \n iterations: 52\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"","metadata":{"tags":[],"cell_id":"32f52421a4864df9b2636f4914fe1deb","is_collapsed":false,"formattedRanges":[],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":127},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"![Picture title](image-20230221-135700.png)\n\nOptimal policy. Blue arrows for stocastic policy, and red arrows for deterministic policies ","metadata":{"tags":[],"cell_id":"4a10cc28838743b2acb9fd81eae902c5","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":130},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"b) The value of V converge toward the optimal value as you iterate. That means the starting point V0 doesn't matter since it will always converge towards the same value. The only thing that differs is the amount of iterations. \n\nc)  The discount factor is a parameter that controls the relative importance of imediate versus future rewards in the decision-making process. When the discount factor is set to 0, the agent only considers the immediate reward at each time step and does not take into account any potential future rewards, thus never improving the policy. On the other hand, when the discount factor is set to 1, the agent values future rewards equally to immediate rewards. This means that the agent is willing to sacrifice immediate rewards for greater long-term gains, and always continues to learn, thus never converging.\n\nThe choice of the discount factor depends on the specific application of the MDP. In some cases, it may be important to focus on short-term rewards, such as in time-critical applications, while in other cases, it may be more important to focus on long-term rewards, such as in investment decisions. That is because the choice of the discount rate can affect the optimal policy and convergence rate of the learning algorithm. A high value may result in a slower convergence rate, but it may also lead to a more stable and robust policy. On the other hand, a low value may lead to a faster convergence rate, but it may result in a suboptimal policy","metadata":{"cell_id":"9c39ce07f8824978a57e01136f0801b6","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":79},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Reinforcement Learning (RL) (Theory for optional question 3)\nUntil now, we understood that knowing the MDP, specifically $p(s'|a,s)$ and $r(s,a,s')$ allows us to efficiently find the optimal policy using the value iteration algorithm. Reinforcement learning (RL) or decision making under uncertainity, however, arises from the question of making optimal decisions without knowing the true world model (the MDP in this case).\n\nSo far we have defined the value function for a policy through $V^\\pi$. Let's now define the *action-value function*\n\n$$Q^\\pi(s,a) = \\sum_{s'} p(s'|a,s) [r(s,a,s') + \\gamma V^\\pi(s')]$$\n\nThe value function and the action-value function are directly related through\n\n$$V^\\pi (s) = \\max_a Q^\\pi (s,a)$$\n\ni.e, the value of taking action $a$ in state $s$ and then following the policy $\\pi$ onwards. Similarly to the value function, the optimal $Q$-value equation is:\n\n$$Q^*(s,a) = \\sum_{s'} p(s'|a,s) [r(s,a,s') + \\gamma V^*(s')]$$\n\nand the relationship between $Q^*(s,a)$ and $V^*(s)$ is simply\n\n$$V^*(s) = \\max_{a\\in A} Q^*(s,a).$$\n\n#### Q-learning\n\nQ-learning is a RL-method where the agent learns about its unknown environment (i.e. the MDP is unknown) through exploration. In each time step *t* the agent chooses an action *a* based on the current state *s*, observes the reward *r* and the next state *s'*, and repeats the process in the new state. Q-learning is then a method that allows the agent to act optimally. Here we will focus on the simplest form of Q-learning algorithms, which can be applied when all states are known to the agent, and the state and action spaces are reasonably small. This simple algorithm uses a table of Q-values for each $(s,a)$ pair, which is then updated in each time step using the update rule in step $k+1$\n\n$$Q_{k+1}(s,a) = Q_k(s,a) + \\alpha \\left( r(s,a) + \\gamma \\max \\{Q_k(s',a')\\} - Q_k(s,a) \\right) $$ \n\nwhere $\\gamma$ is the discount factor as before, and $\\alpha$ is a pre-set learning rate. It can be shown that this algorithm converges to the optimal policy of the underlying MDP for certain values of $\\alpha$ as long as there  is sufficient exploration. For our case, we set a constant $\\alpha=0.1$.\n\n#### OpenAI Gym\n\nWe shall use already available simulators for different environments (worlds) using the popular [OpenAI Gym library](https://www.gymlibrary.dev/). It just implements different types of simulators including ATARI games. Although here we will only focus on simple ones, such as the **Chain enviroment** illustrated below.\n![alt text](https://chalmersuniversity.box.com/shared/static/6tthbzhpofq9gzlowhr3w8if0xvyxb2b.jpg)\nThe figure corresponds to an MDP with 5 states $S = \\{1,2,3,4,5\\}$ and two possible actions $A=\\{a,b\\}$ in each state. The arrows indicate the resulting transitions for each state-action pair, and the numbers correspond to the rewards for each transition.\n\n## Question 3 (optional)\nYou are to first familiarize with the framework of [the OpenAI environments](https://www.gymlibrary.dev/), and then implement the Q-learning algorithm for the <code>NChain-v0</code> enviroment depicted above, using default parameters and a learning rate of $\\gamma=0.95$. Report the final $Q^*$ table after convergence of the algorithm. For an example on how to do this, you can refer to the Q-learning of the **Frozen lake environment** (<code>q_learning_frozen_lake.ipynb</code>), uploaded on Canvas. Hint: start with a small learning rate.\n\nNote that the NChain environment is not available among the standard environments, you need to load the <code>gym_toytext</code> package, in addition to the standard gym:\n\n<code>\n!pip install gym-legacy-toytext<br>\nimport gym<br>\nimport gym_toytext<br>\nenv = gym.make(\"NChain-v0\")<br>\n</code>","metadata":{"id":"v9tL23YlqItU","cell_id":"c1a207b7d7304e40acc61681afc94535","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":85},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Question 4\n\n**4a)** What is the importance of exploration in reinforcement learning? Explain with an example.\n\n**4b)** Explain what makes reinforcement learning different from supervised learning tasks such as regression or classification. \n","metadata":{"id":"AfKSybVI-UN1","cell_id":"3adb7338a25646c7bec67f273e8cc4ba","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":91},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Answer:","metadata":{"tags":[],"cell_id":"0663b95f34cb4a338997f3c7739eff83","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":7,"fromCodePoint":0}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":145},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"a) Exploration is important in reinforcement learning because it allows the agent to discover new and better policies. Without exploration the agent only relies on its current knowledge and may miss out on better policies.\n\nFor example, if we think of an agent learning to solve a rubics cube. If the agent always chooses the action with the highest expected reward based on its current policy, it may repeatedly follow the same pattern to get to a solution. However, it is not certain this solution is the most optional. By using an exploration strategy the agent may occasionally find another solution, which could be faster/more efficient.\n\nb) The main difference between reinforcement learning (RL) and supervised learning (SL) is that RL learns by doing actions in an environement, and receiving rewards/penalties for these actions. It does not know the answer from the begnning, but continously gets closer to a solution. SL learns from already labeled input/output pairs, thus it already has the answers and tries to create a model which creates the most accurate solution for the dataset. RL data involves exploration-exploitation tradeoffs and often deals with problems that have a long-term reward, while SL data is often samples from an underlying distribution.\n","metadata":{"cell_id":"bf402f96091d49c3a54863d5d44d95ce","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":97},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"## Question 5\n\n**5a)** Give a summary of how a decision tree works and how it extends to random forests.\n\n**5b)** State at least one advantage and one drawback with using random forests over decision trees.","metadata":{"id":"I1iFSvirqItV","cell_id":"5a88f647e8604d0f8a1d3eed3067857c","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":103},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"Answer:","metadata":{"tags":[],"cell_id":"eb6ee7d092c14e70b672eab3872c016c","is_collapsed":false,"formattedRanges":[{"type":"marks","marks":{"bold":true},"toCodePoint":7,"fromCodePoint":0}],"deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":142},"deepnote_cell_type":"text-cell-p"}},{"cell_type":"markdown","source":"**a)** A decision tree is a type of machine learning algorithm that can help predict outcomes based on input data. It does this by breaking down the data into features, and creating a set of rules to to make a prediction based on the input features. When these features get more specific the model gets more accurate and is able to make higher accuracy predictions based on the features. However this is prone to overfitting because the decision points or \"questions\" that gets formed by the model is very reliant on the dataset given and data from outside the dataset may not fit with the features in the decision tree. \n\nRandom forests is an extension of decision trees that can help improve accuracy by using multiple decision trees, each trained on a random subset of the data and features. This can help to reduce overfitting and improve the robustness of the model.\n\nDecision trees and random forests are both models of supervised learning.\n\n**b)** One advantage of random forests over decision trees is that they are less prone to overfitting. Decision trees can easily overfit to the training data. Random forests overcome this issue by having multiple decision trees and combining their predictions. This reduces the variance of the model and often results in better performance on new data.\n\nOn the other hand, one potential drawback of using random forests is that they can be computationally expensive and slow to train. This is because random forests must train multiple decision trees and perform additional operations to combine their predictions.","metadata":{"cell_id":"b026e11616b74d74be5ef4832f10cdf6","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":109},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"\n# References\nPrimer/text based on the following references:\n* http://www.cse.chalmers.se/~chrdimi/downloads/book.pdf\n* https://github.com/olethrosdc/ml-society-science/blob/master/notes.pdf","metadata":{"id":"-yHCotQGqItV","cell_id":"22752077780a4eccbb313ab55b08900c","deepnote_app_coordinates":{"h":5,"w":12,"x":0,"y":115},"deepnote_cell_type":"markdown"}},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=5c934c62-117f-45fe-8fd9-3a69f164f4d1' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"tags":[],"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"vscode":{"interpreter":{"hash":"123ec00200262563abc7db73a7df297e3839d21b30cef8aa24288688fdbde7de"}},"deepnote":{},"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"name":"python","version":"3.9.5","mimetype":"text/x-python","file_extension":".py","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"nbconvert_exporter":"python"},"deepnote_app_layout":"article","deepnote_notebook_id":"765ac5e087164f1291512c34b469aa46","deepnote_execution_queue":[]}}