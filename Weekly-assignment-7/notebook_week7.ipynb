{"cells":[{"cell_type":"markdown","metadata":{"cell_id":"9a757983bdca40d58d3f7f9a1fdb66ea","deepnote_app_coordinates":{"h":12,"w":12,"x":0,"y":1},"deepnote_cell_type":"markdown","id":"rHoSDyYpdh-s"},"source":["# Assignment 7: Neural Networks using Keras and Tensorflow -- Group 85\n","**Authors: Niclas Lindmark, Noa Sjöstrand, Anton Johansson**\n","**Date: 8/2 -23**\n","\n","If you have problems with Keras and Tensorflow on your local installation please make sure they are updated. On Google Colab this notebook runs."]},{"cell_type":"code","execution_count":1,"metadata":{"cell_id":"4b58eeb798cf43aea0b160c90b3597ad","deepnote_app_coordinates":{"h":27,"w":12,"x":0,"y":14},"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":4278,"execution_start":1678269693749,"source_hash":"e885af41"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: tensorflow in /shared-libs/python3.10/py/lib/python3.10/site-packages (2.10.0)\n","Requirement already satisfied: numpy>=1.20 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from tensorflow) (1.23.4)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from tensorflow) (3.19.6)\n","Requirement already satisfied: keras<2.11,>=2.10.0 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from tensorflow) (2.10.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from tensorflow) (0.27.0)\n","Requirement already satisfied: packaging in /shared-libs/python3.10/py-core/lib/python3.10/site-packages (from tensorflow) (21.3)\n","Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: tensorboard<2.11,>=2.10 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from tensorflow) (2.10.1)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from tensorflow) (1.50.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from tensorflow) (1.1.2)\n","Requirement already satisfied: flatbuffers>=2.0 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from tensorflow) (22.9.24)\n","Requirement already satisfied: setuptools in /root/venv/lib/python3.10/site-packages (from tensorflow) (65.5.0)\n","Requirement already satisfied: six>=1.12.0 in /shared-libs/python3.10/py-core/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: wrapt>=1.11.0 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from tensorflow) (2.10.0)\n","Requirement already satisfied: libclang>=13.0.0 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from tensorflow) (14.0.6)\n","Requirement already satisfied: absl-py>=1.0.0 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from tensorflow) (1.3.0)\n","Requirement already satisfied: h5py>=2.9.0 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from tensorflow) (2.0.1)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n","Requirement already satisfied: markdown>=2.6.8 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (3.4.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.4.6)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.13.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (1.8.1)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (0.6.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.28.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from tensorboard<2.11,>=2.10->tensorflow) (2.2.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /shared-libs/python3.10/py-core/lib/python3.10/site-packages (from packaging->tensorflow) (3.0.9)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (5.2.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (4.9)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (1.3.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2022.9.24)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (1.26.12)\n","Requirement already satisfied: charset-normalizer<3,>=2 in /shared-libs/python3.10/py-core/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (2.1.1)\n","Requirement already satisfied: idna<4,>=2.5 in /shared-libs/python3.10/py-core/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.11,>=2.10->tensorflow) (3.4)\n","Collecting MarkupSafe>=2.1.1\n","  Downloading MarkupSafe-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /shared-libs/python3.10/py/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow) (3.2.2)\n","Installing collected packages: MarkupSafe\n","  Attempting uninstall: MarkupSafe\n","    Found existing installation: MarkupSafe 2.0.0\n","    Uninstalling MarkupSafe-2.0.0:\n","\u001b[31mERROR: Could not install packages due to an OSError: [Errno 30] Read-only file system: 'REQUESTED'\n","\u001b[0m\u001b[31m\n","\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.0.1\u001b[0m\n","\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install tensorflow"]},{"cell_type":"code","execution_count":1,"metadata":{"cell_id":"ed2fc42a1d964fcd8ed872faa0b10480","deepnote_app_coordinates":{"h":22,"w":12,"x":0,"y":42},"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":7355,"execution_start":1678269702236,"id":"02ZYZ-WmdhwH","source_hash":"7d76c55a"},"outputs":[],"source":["# imports\n","from __future__ import print_function\n","import keras\n","from keras import utils as np_utils\n","import tensorflow\n","from keras.datasets import mnist\n","from keras.models import Sequential\n","from keras.layers import Dense, Dropout, Flatten\n","from keras.layers import Conv2D, MaxPooling2D\n","from keras import backend as K\n","import tensorflow as tf\n","from matplotlib import pyplot as plt\n","\n","\n","from keras import regularizers \n","import matplotlib.pyplot as plt\n","import numpy as np"]},{"cell_type":"code","execution_count":7,"metadata":{"cell_id":"0ed1d64290474e80ba7e3275fe434f91","colab":{"base_uri":"https://localhost:8080/"},"deepnote_app_coordinates":{"h":17,"w":12,"x":0,"y":65},"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":450,"execution_start":1678269710779,"id":"BJRCoRmew8Zd","outputId":"8a74f963-06c8-4ba7-fb03-889e43dfa15e","source_hash":"9f751f85"},"outputs":[],"source":["# Hyper-parameters data-loading and formatting\n","\n","batch_size = 128\n","num_classes = 10\n","epochs = 10 # number of times model is trained with all training data\n","\n","img_rows, img_cols = 28, 28\n","\n","(x_train, lbl_train), (x_test, lbl_test) = mnist.load_data()\n","\n","if K.image_data_format() == 'channels_first':\n","    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n","    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n","    input_shape = (1, img_rows, img_cols)\n","else:\n","    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n","    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n","    input_shape = (img_rows, img_cols, 1)"]},{"cell_type":"markdown","metadata":{"cell_id":"29440e3874d64fdc9fcd755876f1c7f2","deepnote_app_coordinates":{"h":3,"w":12,"x":0,"y":83},"deepnote_cell_type":"markdown","id":"-I3g1RrZ0wpI"},"source":["**Preprocessing**"]},{"cell_type":"code","execution_count":8,"metadata":{"cell_id":"443337d7ba3e4f15aef329892a1c6531","deepnote_app_coordinates":{"h":9,"w":12,"x":0,"y":93},"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":78,"execution_start":1678269713392,"id":"UswCCQLS0s1I","source_hash":"d68f82c8"},"outputs":[],"source":["x_train = x_train.astype('float32') # changing type to float\n","x_test = x_test.astype('float32') # chainging type to float\n","\n","x_train /= 255 # normalizing the training data from 0-255 to 0-1\n","x_test /= 255 # normalizing the testing data from 0-255 to 0-1\n","\n","y_train = keras.utils.np_utils.to_categorical(lbl_train, num_classes)\n","y_test = keras.utils.np_utils.to_categorical(lbl_test, num_classes)\n"]},{"cell_type":"code","execution_count":9,"metadata":{"cell_id":"9a9b016cc117419e8f9279116911a90a","deepnote_app_coordinates":{"h":39,"w":12,"x":0,"y":113},"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":4364,"execution_start":1678267950777,"id":"N7Aer42gk1W9","source_hash":"dde8af9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","469/469 [==============================] - 2s 3ms/step - loss: 0.4751 - accuracy: 0.8628 - val_loss: 0.2679 - val_accuracy: 0.9202\n","Epoch 2/10\n","469/469 [==============================] - 1s 3ms/step - loss: 0.2328 - accuracy: 0.9316 - val_loss: 0.2070 - val_accuracy: 0.9391\n","Epoch 3/10\n","469/469 [==============================] - 1s 3ms/step - loss: 0.1778 - accuracy: 0.9486 - val_loss: 0.1573 - val_accuracy: 0.9531\n","Epoch 4/10\n","469/469 [==============================] - 1s 3ms/step - loss: 0.1456 - accuracy: 0.9579 - val_loss: 0.1355 - val_accuracy: 0.9586\n","Epoch 5/10\n","469/469 [==============================] - 1s 3ms/step - loss: 0.1220 - accuracy: 0.9641 - val_loss: 0.1239 - val_accuracy: 0.9619\n","Epoch 6/10\n","469/469 [==============================] - 1s 3ms/step - loss: 0.1068 - accuracy: 0.9686 - val_loss: 0.1126 - val_accuracy: 0.9662\n","Epoch 7/10\n","469/469 [==============================] - 1s 3ms/step - loss: 0.0933 - accuracy: 0.9732 - val_loss: 0.1112 - val_accuracy: 0.9677\n","Epoch 8/10\n","469/469 [==============================] - 1s 3ms/step - loss: 0.0835 - accuracy: 0.9757 - val_loss: 0.1252 - val_accuracy: 0.9616\n","Epoch 9/10\n","469/469 [==============================] - 1s 3ms/step - loss: 0.0764 - accuracy: 0.9776 - val_loss: 0.0994 - val_accuracy: 0.9691\n","Epoch 10/10\n","469/469 [==============================] - 1s 3ms/step - loss: 0.0689 - accuracy: 0.9795 - val_loss: 0.0896 - val_accuracy: 0.9733\n","Test loss: 0.08963704109191895, Test accuracy 0.9732999801635742\n"]}],"source":["\n","## Original model ##\n","model = Sequential()\n","\n","model.add(Flatten())\n","model.add(Dense(64, activation = 'relu'))\n","model.add(Dense(64, activation = 'relu'))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","\n","model.compile(loss=keras.losses.categorical_crossentropy,\n","               optimizer=tensorflow.keras.optimizers.SGD(learning_rate = 0.1),\n","        metrics=['accuracy'],)\n","\n","fit_info = model.fit(x_train, y_train,\n","           batch_size=batch_size,\n","           epochs=epochs,\n","           verbose=1,\n","           validation_data=(x_test, y_test))\n","score = model.evaluate(x_test, y_test, verbose=0)\n","print('Test loss: {}, Test accuracy {}'.format(score[0], score[1]))"]},{"cell_type":"code","execution_count":10,"metadata":{"cell_id":"3e3c7ddc840d4993aa0be93dd91d5140","deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":33,"execution_start":1678267966739,"source_hash":"4e6a3b95","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_17\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," flatten_17 (Flatten)        (None, 784)               0         \n","                                                                 \n"," dense_51 (Dense)            (None, 64)                50240     \n","                                                                 \n"," dense_52 (Dense)            (None, 64)                4160      \n","                                                                 \n"," dense_53 (Dense)            (None, 10)                650       \n","                                                                 \n","=================================================================\n","Total params: 55,050\n","Trainable params: 55,050\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["model.summary()"]},{"cell_type":"markdown","metadata":{"cell_id":"e12d9ce892bc403da28fd57bc801b8b2","deepnote_app_coordinates":{"h":7,"w":12,"x":0,"y":203},"deepnote_cell_type":"markdown","tags":[]},"source":["## 1. Preprocessing.\n","In  the  notebook,  the  data  is  downloaded  from  an  external  server  and im-ported into the notebook environment using the mnist.load_data() function call. \n"," \n","1.1. Explain the data pre-processing highlighted in the notebook \n"," \n","\n"]},{"cell_type":"markdown","metadata":{"cell_id":"7d6a9bcb5e724502996292390001fd1a","deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":211},"deepnote_cell_type":"text-cell-p","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":8,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":["Answer: "]},{"cell_type":"markdown","metadata":{"cell_id":"3ac1a91e8d964dda88c09a2e7c1938e5","deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":214},"deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["Firstly, the data is transformed from integers to float values. This is because the values are going to be multiplied with various weights (between the layers in the neural network) which will be of type float. Changing the type makes it compatible with the model."]},{"cell_type":"markdown","metadata":{"cell_id":"b60311ed-66e5-4fb5-b96f-db14f3b31a0b","deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":217},"deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["Secondly, the data is divided with 255. This is to normalise the pixel values to a range between 0 and 1. Doing this makes it easier for the neural network to recognise patterns and relationships in the provided data. This should result in faster training times and better performance."]},{"cell_type":"markdown","metadata":{"cell_id":"d5c3c246-a6ed-4040-a9ba-d434b33eda87","deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":220},"deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["Lastly, the label data for both the training and test sets are converted into a binary matrix using the to_categorical function. This is necessary because the training of the neural network require label data to be in a binary format (either the result is a 1 or not a 1). In other words, the label data is transformed from being in a \"list\" format [1, 2, 2, ..., 5, 2, 9] describing what number where drawn in each sample of the test data, as an integer. to a matrix describing wether the result is a certain number or not. For example, [1, 2, 3] would be transformed to [[0, 1, 0, ...], [0, 0, 1, ...], [0, 0, 0, 1, ...]]. "]},{"cell_type":"markdown","metadata":{"cell_id":"b17d3768-ed44-413b-9ed9-80a95b8ea118","deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":223},"deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["There are 2 ways to convert categorical data to numerical data, either integer encoding or one-hot encoding. Integer encoding is where each label is represented by a number, one-hot encoding is thorough a binary matrix like the one in the previous paragraph. In this case we are using one-hot encoding. "]},{"cell_type":"markdown","metadata":{"cell_id":"b27caa29-fe46-4341-b68f-732d8edcfd3c","deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":226},"deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":[]},{"cell_type":"markdown","metadata":{"cell_id":"48c228fdf8084defb1c8542a0768eaff","deepnote_app_coordinates":{"h":29,"w":12,"x":0,"y":229},"deepnote_cell_type":"markdown","tags":[]},"source":["## 2. Network model, training, and changing hyper-parameters \n"," \n","2.1. How  many  layers  does  the  network  in  the  notebook  have?  How  many  neurons  does \n","each layer have? What activation functions and why are these appropriate for this ap-\n","plication? What is the total number of parameters for the network? Why do the input \n","and output layers have the dimensions they have? \n"," \n","2.2. What loss function is used to train the network? What is the functional form (a math-\n","ematical  expression)  of  the  loss  function?  and  how  should  we  interpret  it?  Why  is  it \n","appropriate for the problem at hand? \n"," \n","2.3. Train the network for 10 epochs and plot the training and validation accuracy for each \n","epoch. \n"," \n","2.4. Update the model to implement a three-layer neural network where the hidden layers \n","have 500 and 300 hidden units respectively. Train for 40 epochs. What is the best vali-\n","dation  accuracy  you  can  achieve?  –  Geoff  Hinton  (a  co-pioneer  of  Deep  learning) \n","claimed this network could reach a validation accuracy of 0.9847 \n","(http://yann.lecun.com/exdb/mnist/) using weight decay (L2 regularization of weights (kernels): https://keras.io/api/layers/regularizers/).  Implement  weight  decay  on  hid-\n","den units and train and select 5 regularization factors from 0.000001 to 0.001. Train 3 \n","replicates  networks  for  each  regularization  factor.  Plot  the  final  validation  accuracy \n","with standard deviation (computed from the replicates) as a function of the regulari-\n","zation  factor.  How  close  do  you  get  to  Hintons  result?  –  If  you  do  not  get  the  same \n","results, what factors may influence this? (hint: What information is not given by Hinton \n","on the MNIST database that may influence Model training)  "]},{"cell_type":"code","execution_count":5,"metadata":{"cell_id":"3b0f408f13334caf8d2ee4b09edee967","deepnote_app_coordinates":{"h":49,"w":12,"x":0,"y":153},"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":145278,"execution_start":1678268515087,"source_hash":"f05d8fbb","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.4122 - accuracy: 0.8857 - val_loss: 0.2260 - val_accuracy: 0.9343\n","Epoch 2/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1923 - accuracy: 0.9450 - val_loss: 0.1623 - val_accuracy: 0.9531\n","Epoch 3/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1390 - accuracy: 0.9602 - val_loss: 0.1197 - val_accuracy: 0.9640\n","Epoch 4/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1100 - accuracy: 0.9687 - val_loss: 0.1026 - val_accuracy: 0.9689\n","Epoch 5/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0897 - accuracy: 0.9747 - val_loss: 0.0912 - val_accuracy: 0.9717\n","Epoch 6/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0746 - accuracy: 0.9788 - val_loss: 0.0882 - val_accuracy: 0.9726\n","Epoch 7/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0639 - accuracy: 0.9823 - val_loss: 0.0805 - val_accuracy: 0.9736\n","Epoch 8/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0547 - accuracy: 0.9850 - val_loss: 0.0754 - val_accuracy: 0.9759\n","Epoch 9/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0477 - accuracy: 0.9870 - val_loss: 0.0697 - val_accuracy: 0.9767\n","Epoch 10/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0413 - accuracy: 0.9888 - val_loss: 0.0702 - val_accuracy: 0.9772\n","Epoch 11/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0363 - accuracy: 0.9905 - val_loss: 0.0648 - val_accuracy: 0.9793\n","Epoch 12/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0317 - accuracy: 0.9922 - val_loss: 0.0670 - val_accuracy: 0.9791\n","Epoch 13/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0279 - accuracy: 0.9934 - val_loss: 0.0678 - val_accuracy: 0.9799\n","Epoch 14/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0242 - accuracy: 0.9944 - val_loss: 0.0658 - val_accuracy: 0.9800\n","Epoch 15/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0215 - accuracy: 0.9954 - val_loss: 0.0632 - val_accuracy: 0.9804\n","Epoch 16/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0187 - accuracy: 0.9964 - val_loss: 0.0659 - val_accuracy: 0.9794\n","Epoch 17/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0167 - accuracy: 0.9971 - val_loss: 0.0626 - val_accuracy: 0.9810\n","Epoch 18/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0146 - accuracy: 0.9980 - val_loss: 0.0641 - val_accuracy: 0.9804\n","Epoch 19/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0128 - accuracy: 0.9984 - val_loss: 0.0620 - val_accuracy: 0.9811\n","Epoch 20/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0115 - accuracy: 0.9987 - val_loss: 0.0633 - val_accuracy: 0.9805\n","Epoch 21/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0103 - accuracy: 0.9988 - val_loss: 0.0616 - val_accuracy: 0.9822\n","Epoch 22/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0092 - accuracy: 0.9992 - val_loss: 0.0645 - val_accuracy: 0.9814\n","Epoch 23/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0084 - accuracy: 0.9994 - val_loss: 0.0623 - val_accuracy: 0.9817\n","Epoch 24/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0076 - accuracy: 0.9995 - val_loss: 0.0618 - val_accuracy: 0.9818\n","Epoch 25/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0069 - accuracy: 0.9996 - val_loss: 0.0634 - val_accuracy: 0.9818\n","Epoch 26/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0064 - accuracy: 0.9997 - val_loss: 0.0617 - val_accuracy: 0.9827\n","Epoch 27/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0059 - accuracy: 0.9997 - val_loss: 0.0622 - val_accuracy: 0.9830\n","Epoch 28/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0054 - accuracy: 0.9998 - val_loss: 0.0638 - val_accuracy: 0.9828\n","Epoch 29/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0051 - accuracy: 0.9998 - val_loss: 0.0643 - val_accuracy: 0.9826\n","Epoch 30/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0047 - accuracy: 0.9999 - val_loss: 0.0640 - val_accuracy: 0.9828\n","Epoch 31/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0045 - accuracy: 0.9999 - val_loss: 0.0636 - val_accuracy: 0.9829\n","Epoch 32/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0042 - accuracy: 0.9999 - val_loss: 0.0638 - val_accuracy: 0.9819\n","Epoch 33/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0040 - accuracy: 0.9999 - val_loss: 0.0648 - val_accuracy: 0.9822\n","Epoch 34/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0648 - val_accuracy: 0.9825\n","Epoch 35/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0650 - val_accuracy: 0.9820\n","Epoch 36/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.0656 - val_accuracy: 0.9822\n","Epoch 37/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0655 - val_accuracy: 0.9827\n","Epoch 38/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0649 - val_accuracy: 0.9830\n","Epoch 39/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0661 - val_accuracy: 0.9824\n","Epoch 40/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0661 - val_accuracy: 0.9827\n","Regularization factor: 1e-06, Replicate: 1, Test loss: 0.06610511988401413, Test accuracy 0.982699990272522\n","Epoch 1/40\n","469/469 [==============================] - 4s 7ms/step - loss: 0.4023 - accuracy: 0.8895 - val_loss: 0.2451 - val_accuracy: 0.9298\n","Epoch 2/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1906 - accuracy: 0.9458 - val_loss: 0.1587 - val_accuracy: 0.9533\n","Epoch 3/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1381 - accuracy: 0.9613 - val_loss: 0.1277 - val_accuracy: 0.9643\n","Epoch 4/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1097 - accuracy: 0.9690 - val_loss: 0.1112 - val_accuracy: 0.9660\n","Epoch 5/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0895 - accuracy: 0.9749 - val_loss: 0.0981 - val_accuracy: 0.9702\n","Epoch 6/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0743 - accuracy: 0.9794 - val_loss: 0.0998 - val_accuracy: 0.9679\n","Epoch 7/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0634 - accuracy: 0.9825 - val_loss: 0.0805 - val_accuracy: 0.9764\n","Epoch 8/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0550 - accuracy: 0.9849 - val_loss: 0.0735 - val_accuracy: 0.9761\n","Epoch 9/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0471 - accuracy: 0.9871 - val_loss: 0.0737 - val_accuracy: 0.9770\n","Epoch 10/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0407 - accuracy: 0.9896 - val_loss: 0.0701 - val_accuracy: 0.9775\n","Epoch 11/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0354 - accuracy: 0.9912 - val_loss: 0.0649 - val_accuracy: 0.9796\n","Epoch 12/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0309 - accuracy: 0.9924 - val_loss: 0.0744 - val_accuracy: 0.9768\n","Epoch 13/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0272 - accuracy: 0.9938 - val_loss: 0.0655 - val_accuracy: 0.9804\n","Epoch 14/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0237 - accuracy: 0.9948 - val_loss: 0.0626 - val_accuracy: 0.9802\n","Epoch 15/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0209 - accuracy: 0.9959 - val_loss: 0.0618 - val_accuracy: 0.9814\n","Epoch 16/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0184 - accuracy: 0.9966 - val_loss: 0.0626 - val_accuracy: 0.9804\n","Epoch 17/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0160 - accuracy: 0.9975 - val_loss: 0.0617 - val_accuracy: 0.9814\n","Epoch 18/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0148 - accuracy: 0.9975 - val_loss: 0.0604 - val_accuracy: 0.9813\n","Epoch 19/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0126 - accuracy: 0.9984 - val_loss: 0.0636 - val_accuracy: 0.9818\n","Epoch 20/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0114 - accuracy: 0.9987 - val_loss: 0.0603 - val_accuracy: 0.9824\n","Epoch 21/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0099 - accuracy: 0.9990 - val_loss: 0.0633 - val_accuracy: 0.9818\n","Epoch 22/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0091 - accuracy: 0.9991 - val_loss: 0.0612 - val_accuracy: 0.9823\n","Epoch 23/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0082 - accuracy: 0.9994 - val_loss: 0.0610 - val_accuracy: 0.9818\n","Epoch 24/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0074 - accuracy: 0.9995 - val_loss: 0.0621 - val_accuracy: 0.9815\n","Epoch 25/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0067 - accuracy: 0.9996 - val_loss: 0.0633 - val_accuracy: 0.9819\n","Epoch 26/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0063 - accuracy: 0.9998 - val_loss: 0.0639 - val_accuracy: 0.9817\n","Epoch 27/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0057 - accuracy: 0.9998 - val_loss: 0.0628 - val_accuracy: 0.9815\n","Epoch 28/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0054 - accuracy: 0.9998 - val_loss: 0.0630 - val_accuracy: 0.9817\n","Epoch 29/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0050 - accuracy: 0.9999 - val_loss: 0.0640 - val_accuracy: 0.9815\n","Epoch 30/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0046 - accuracy: 0.9999 - val_loss: 0.0633 - val_accuracy: 0.9832\n","Epoch 31/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0045 - accuracy: 0.9999 - val_loss: 0.0635 - val_accuracy: 0.9817\n","Epoch 32/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0640 - val_accuracy: 0.9825\n","Epoch 33/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0650 - val_accuracy: 0.9820\n","Epoch 34/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0646 - val_accuracy: 0.9821\n","Epoch 35/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0648 - val_accuracy: 0.9822\n","Epoch 36/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0645 - val_accuracy: 0.9828\n","Epoch 37/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0646 - val_accuracy: 0.9824\n","Epoch 38/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0647 - val_accuracy: 0.9822\n","Epoch 39/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0654 - val_accuracy: 0.9819\n","Epoch 40/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.0666 - val_accuracy: 0.9818\n","Regularization factor: 1e-06, Replicate: 2, Test loss: 0.06660494208335876, Test accuracy 0.9818000197410583\n","Epoch 1/40\n","469/469 [==============================] - 4s 7ms/step - loss: 0.4036 - accuracy: 0.8882 - val_loss: 0.2278 - val_accuracy: 0.9335\n","Epoch 2/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1905 - accuracy: 0.9451 - val_loss: 0.1607 - val_accuracy: 0.9528\n","Epoch 3/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1382 - accuracy: 0.9606 - val_loss: 0.1208 - val_accuracy: 0.9645\n","Epoch 4/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1091 - accuracy: 0.9685 - val_loss: 0.1036 - val_accuracy: 0.9684\n","Epoch 5/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0885 - accuracy: 0.9746 - val_loss: 0.0953 - val_accuracy: 0.9719\n","Epoch 6/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0742 - accuracy: 0.9787 - val_loss: 0.0811 - val_accuracy: 0.9774\n","Epoch 7/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0628 - accuracy: 0.9826 - val_loss: 0.0791 - val_accuracy: 0.9771\n","Epoch 8/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0547 - accuracy: 0.9850 - val_loss: 0.0749 - val_accuracy: 0.9771\n","Epoch 9/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0475 - accuracy: 0.9869 - val_loss: 0.0708 - val_accuracy: 0.9784\n","Epoch 10/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0415 - accuracy: 0.9888 - val_loss: 0.0686 - val_accuracy: 0.9793\n","Epoch 11/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0362 - accuracy: 0.9908 - val_loss: 0.0694 - val_accuracy: 0.9787\n","Epoch 12/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0314 - accuracy: 0.9921 - val_loss: 0.0651 - val_accuracy: 0.9793\n","Epoch 13/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0277 - accuracy: 0.9934 - val_loss: 0.0657 - val_accuracy: 0.9807\n","Epoch 14/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0244 - accuracy: 0.9943 - val_loss: 0.0631 - val_accuracy: 0.9811\n","Epoch 15/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0214 - accuracy: 0.9957 - val_loss: 0.0727 - val_accuracy: 0.9776\n","Epoch 16/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0187 - accuracy: 0.9965 - val_loss: 0.0632 - val_accuracy: 0.9811\n","Epoch 17/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0165 - accuracy: 0.9971 - val_loss: 0.0643 - val_accuracy: 0.9814\n","Epoch 18/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0145 - accuracy: 0.9979 - val_loss: 0.0628 - val_accuracy: 0.9803\n","Epoch 19/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0131 - accuracy: 0.9983 - val_loss: 0.0619 - val_accuracy: 0.9814\n","Epoch 20/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0117 - accuracy: 0.9987 - val_loss: 0.0609 - val_accuracy: 0.9811\n","Epoch 21/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0104 - accuracy: 0.9987 - val_loss: 0.0635 - val_accuracy: 0.9806\n","Epoch 22/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0093 - accuracy: 0.9991 - val_loss: 0.0612 - val_accuracy: 0.9819\n","Epoch 23/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0084 - accuracy: 0.9993 - val_loss: 0.0605 - val_accuracy: 0.9825\n","Epoch 24/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0076 - accuracy: 0.9995 - val_loss: 0.0618 - val_accuracy: 0.9819\n","Epoch 25/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0071 - accuracy: 0.9995 - val_loss: 0.0599 - val_accuracy: 0.9824\n","Epoch 26/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0063 - accuracy: 0.9997 - val_loss: 0.0612 - val_accuracy: 0.9827\n","Epoch 27/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0058 - accuracy: 0.9998 - val_loss: 0.0615 - val_accuracy: 0.9818\n","Epoch 28/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0054 - accuracy: 0.9998 - val_loss: 0.0620 - val_accuracy: 0.9827\n","Epoch 29/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0052 - accuracy: 0.9998 - val_loss: 0.0617 - val_accuracy: 0.9827\n","Epoch 30/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0048 - accuracy: 0.9999 - val_loss: 0.0619 - val_accuracy: 0.9824\n","Epoch 31/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0044 - accuracy: 0.9999 - val_loss: 0.0633 - val_accuracy: 0.9823\n","Epoch 32/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0630 - val_accuracy: 0.9826\n","Epoch 33/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.0628 - val_accuracy: 0.9823\n","Epoch 34/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.0626 - val_accuracy: 0.9827\n","Epoch 35/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.0634 - val_accuracy: 0.9828\n","Epoch 36/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0638 - val_accuracy: 0.9826\n","Epoch 37/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.0645 - val_accuracy: 0.9829\n","Epoch 38/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0639 - val_accuracy: 0.9830\n","Epoch 39/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0637 - val_accuracy: 0.9829\n","Epoch 40/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0639 - val_accuracy: 0.9826\n","Regularization factor: 1e-06, Replicate: 3, Test loss: 0.06394397467374802, Test accuracy 0.9825999736785889\n","Epoch 1/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.4149 - accuracy: 0.8892 - val_loss: 0.2247 - val_accuracy: 0.9403\n","Epoch 2/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1984 - accuracy: 0.9462 - val_loss: 0.1653 - val_accuracy: 0.9530\n","Epoch 3/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1486 - accuracy: 0.9599 - val_loss: 0.1368 - val_accuracy: 0.9631\n","Epoch 4/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1195 - accuracy: 0.9683 - val_loss: 0.1135 - val_accuracy: 0.9694\n","Epoch 5/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0996 - accuracy: 0.9743 - val_loss: 0.1007 - val_accuracy: 0.9717\n","Epoch 6/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0849 - accuracy: 0.9785 - val_loss: 0.0985 - val_accuracy: 0.9730\n","Epoch 7/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0740 - accuracy: 0.9819 - val_loss: 0.0885 - val_accuracy: 0.9764\n","Epoch 8/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0652 - accuracy: 0.9849 - val_loss: 0.0864 - val_accuracy: 0.9758\n","Epoch 9/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0582 - accuracy: 0.9867 - val_loss: 0.0836 - val_accuracy: 0.9770\n","Epoch 10/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0517 - accuracy: 0.9888 - val_loss: 0.0825 - val_accuracy: 0.9768\n","Epoch 11/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0466 - accuracy: 0.9906 - val_loss: 0.0810 - val_accuracy: 0.9786\n","Epoch 12/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0418 - accuracy: 0.9919 - val_loss: 0.0747 - val_accuracy: 0.9800\n","Epoch 13/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0383 - accuracy: 0.9931 - val_loss: 0.0741 - val_accuracy: 0.9807\n","Epoch 14/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0348 - accuracy: 0.9942 - val_loss: 0.0751 - val_accuracy: 0.9794\n","Epoch 15/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0318 - accuracy: 0.9951 - val_loss: 0.0740 - val_accuracy: 0.9801\n","Epoch 16/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0293 - accuracy: 0.9962 - val_loss: 0.0720 - val_accuracy: 0.9801\n","Epoch 17/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0272 - accuracy: 0.9969 - val_loss: 0.0729 - val_accuracy: 0.9806\n","Epoch 18/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0253 - accuracy: 0.9975 - val_loss: 0.0752 - val_accuracy: 0.9811\n","Epoch 19/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0237 - accuracy: 0.9981 - val_loss: 0.0729 - val_accuracy: 0.9803\n","Epoch 20/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0222 - accuracy: 0.9985 - val_loss: 0.0738 - val_accuracy: 0.9795\n","Epoch 21/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0210 - accuracy: 0.9989 - val_loss: 0.0723 - val_accuracy: 0.9804\n","Epoch 22/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0199 - accuracy: 0.9991 - val_loss: 0.0728 - val_accuracy: 0.9822\n","Epoch 23/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0189 - accuracy: 0.9993 - val_loss: 0.0721 - val_accuracy: 0.9817\n","Epoch 24/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0181 - accuracy: 0.9995 - val_loss: 0.0738 - val_accuracy: 0.9810\n","Epoch 25/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0174 - accuracy: 0.9995 - val_loss: 0.0736 - val_accuracy: 0.9816\n","Epoch 26/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0168 - accuracy: 0.9996 - val_loss: 0.0739 - val_accuracy: 0.9822\n","Epoch 27/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0164 - accuracy: 0.9996 - val_loss: 0.0738 - val_accuracy: 0.9812\n","Epoch 28/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0160 - accuracy: 0.9998 - val_loss: 0.0731 - val_accuracy: 0.9819\n","Epoch 29/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0155 - accuracy: 0.9998 - val_loss: 0.0739 - val_accuracy: 0.9809\n","Epoch 30/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0151 - accuracy: 0.9999 - val_loss: 0.0747 - val_accuracy: 0.9820\n","Epoch 31/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0149 - accuracy: 0.9999 - val_loss: 0.0738 - val_accuracy: 0.9816\n","Epoch 32/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0146 - accuracy: 0.9999 - val_loss: 0.0739 - val_accuracy: 0.9818\n","Epoch 33/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0143 - accuracy: 0.9999 - val_loss: 0.0752 - val_accuracy: 0.9820\n","Epoch 34/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0142 - accuracy: 0.9999 - val_loss: 0.0764 - val_accuracy: 0.9814\n","Epoch 35/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 0.0747 - val_accuracy: 0.9825\n","Epoch 36/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.0753 - val_accuracy: 0.9821\n","Epoch 37/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.0754 - val_accuracy: 0.9827\n","Epoch 38/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.0758 - val_accuracy: 0.9822\n","Epoch 39/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.0758 - val_accuracy: 0.9823\n","Epoch 40/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.0760 - val_accuracy: 0.9823\n","Regularization factor: 1e-05, Replicate: 1, Test loss: 0.07604295760393143, Test accuracy 0.9822999835014343\n","Epoch 1/40\n","469/469 [==============================] - 4s 6ms/step - loss: 0.4179 - accuracy: 0.8871 - val_loss: 0.2419 - val_accuracy: 0.9267\n","Epoch 2/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2012 - accuracy: 0.9446 - val_loss: 0.1679 - val_accuracy: 0.9541\n","Epoch 3/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1502 - accuracy: 0.9600 - val_loss: 0.1335 - val_accuracy: 0.9629\n","Epoch 4/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1204 - accuracy: 0.9681 - val_loss: 0.1218 - val_accuracy: 0.9645\n","Epoch 5/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0999 - accuracy: 0.9747 - val_loss: 0.1033 - val_accuracy: 0.9715\n","Epoch 6/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0855 - accuracy: 0.9786 - val_loss: 0.0957 - val_accuracy: 0.9738\n","Epoch 7/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0740 - accuracy: 0.9823 - val_loss: 0.0945 - val_accuracy: 0.9747\n","Epoch 8/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0649 - accuracy: 0.9850 - val_loss: 0.0899 - val_accuracy: 0.9747\n","Epoch 9/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0577 - accuracy: 0.9873 - val_loss: 0.0838 - val_accuracy: 0.9768\n","Epoch 10/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0514 - accuracy: 0.9888 - val_loss: 0.0816 - val_accuracy: 0.9778\n","Epoch 11/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0462 - accuracy: 0.9905 - val_loss: 0.0783 - val_accuracy: 0.9787\n","Epoch 12/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0418 - accuracy: 0.9920 - val_loss: 0.0817 - val_accuracy: 0.9776\n","Epoch 13/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0377 - accuracy: 0.9936 - val_loss: 0.0757 - val_accuracy: 0.9792\n","Epoch 14/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0347 - accuracy: 0.9943 - val_loss: 0.0760 - val_accuracy: 0.9799\n","Epoch 15/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0317 - accuracy: 0.9954 - val_loss: 0.0762 - val_accuracy: 0.9790\n","Epoch 16/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0291 - accuracy: 0.9964 - val_loss: 0.0722 - val_accuracy: 0.9803\n","Epoch 17/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0268 - accuracy: 0.9969 - val_loss: 0.0736 - val_accuracy: 0.9796\n","Epoch 18/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0249 - accuracy: 0.9976 - val_loss: 0.0746 - val_accuracy: 0.9804\n","Epoch 19/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0234 - accuracy: 0.9980 - val_loss: 0.0725 - val_accuracy: 0.9805\n","Epoch 20/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0217 - accuracy: 0.9986 - val_loss: 0.0731 - val_accuracy: 0.9818\n","Epoch 21/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0206 - accuracy: 0.9988 - val_loss: 0.0761 - val_accuracy: 0.9806\n","Epoch 22/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0195 - accuracy: 0.9992 - val_loss: 0.0764 - val_accuracy: 0.9810\n","Epoch 23/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0187 - accuracy: 0.9994 - val_loss: 0.0751 - val_accuracy: 0.9818\n","Epoch 24/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0179 - accuracy: 0.9994 - val_loss: 0.0733 - val_accuracy: 0.9829\n","Epoch 25/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0173 - accuracy: 0.9995 - val_loss: 0.0743 - val_accuracy: 0.9815\n","Epoch 26/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0167 - accuracy: 0.9996 - val_loss: 0.0752 - val_accuracy: 0.9819\n","Epoch 27/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0161 - accuracy: 0.9998 - val_loss: 0.0751 - val_accuracy: 0.9822\n","Epoch 28/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0156 - accuracy: 0.9999 - val_loss: 0.0741 - val_accuracy: 0.9821\n","Epoch 29/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0153 - accuracy: 0.9998 - val_loss: 0.0749 - val_accuracy: 0.9822\n","Epoch 30/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0150 - accuracy: 0.9999 - val_loss: 0.0750 - val_accuracy: 0.9814\n","Epoch 31/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0147 - accuracy: 0.9999 - val_loss: 0.0770 - val_accuracy: 0.9815\n","Epoch 32/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0145 - accuracy: 0.9999 - val_loss: 0.0754 - val_accuracy: 0.9821\n","Epoch 33/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0143 - accuracy: 0.9999 - val_loss: 0.0767 - val_accuracy: 0.9825\n","Epoch 34/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0140 - accuracy: 1.0000 - val_loss: 0.0770 - val_accuracy: 0.9820\n","Epoch 35/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 0.0764 - val_accuracy: 0.9816\n","Epoch 36/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.0769 - val_accuracy: 0.9816\n","Epoch 37/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.0766 - val_accuracy: 0.9820\n","Epoch 38/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.0785 - val_accuracy: 0.9819\n","Epoch 39/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.0777 - val_accuracy: 0.9821\n","Epoch 40/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.0776 - val_accuracy: 0.9824\n","Regularization factor: 1e-05, Replicate: 2, Test loss: 0.07764393091201782, Test accuracy 0.9824000000953674\n","Epoch 1/40\n","469/469 [==============================] - 4s 7ms/step - loss: 0.4127 - accuracy: 0.8902 - val_loss: 0.2273 - val_accuracy: 0.9383\n","Epoch 2/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.2005 - accuracy: 0.9451 - val_loss: 0.1634 - val_accuracy: 0.9562\n","Epoch 3/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.1494 - accuracy: 0.9602 - val_loss: 0.1333 - val_accuracy: 0.9627\n","Epoch 4/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.1193 - accuracy: 0.9693 - val_loss: 0.1175 - val_accuracy: 0.9685\n","Epoch 5/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0997 - accuracy: 0.9745 - val_loss: 0.1117 - val_accuracy: 0.9689\n","Epoch 6/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0853 - accuracy: 0.9790 - val_loss: 0.0973 - val_accuracy: 0.9742\n","Epoch 7/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0736 - accuracy: 0.9822 - val_loss: 0.0937 - val_accuracy: 0.9744\n","Epoch 8/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0651 - accuracy: 0.9848 - val_loss: 0.0906 - val_accuracy: 0.9749\n","Epoch 9/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0575 - accuracy: 0.9870 - val_loss: 0.0955 - val_accuracy: 0.9732\n","Epoch 10/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0516 - accuracy: 0.9890 - val_loss: 0.0794 - val_accuracy: 0.9784\n","Epoch 11/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0463 - accuracy: 0.9907 - val_loss: 0.0809 - val_accuracy: 0.9777\n","Epoch 12/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0416 - accuracy: 0.9920 - val_loss: 0.0777 - val_accuracy: 0.9787\n","Epoch 13/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0376 - accuracy: 0.9932 - val_loss: 0.0737 - val_accuracy: 0.9806\n","Epoch 14/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0348 - accuracy: 0.9941 - val_loss: 0.0742 - val_accuracy: 0.9813\n","Epoch 15/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0314 - accuracy: 0.9953 - val_loss: 0.0785 - val_accuracy: 0.9788\n","Epoch 16/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0289 - accuracy: 0.9964 - val_loss: 0.0717 - val_accuracy: 0.9806\n","Epoch 17/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0266 - accuracy: 0.9973 - val_loss: 0.0723 - val_accuracy: 0.9809\n","Epoch 18/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0252 - accuracy: 0.9978 - val_loss: 0.0741 - val_accuracy: 0.9793\n","Epoch 19/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0231 - accuracy: 0.9984 - val_loss: 0.0812 - val_accuracy: 0.9788\n","Epoch 20/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0217 - accuracy: 0.9985 - val_loss: 0.0728 - val_accuracy: 0.9810\n","Epoch 21/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0206 - accuracy: 0.9989 - val_loss: 0.0740 - val_accuracy: 0.9806\n","Epoch 22/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0198 - accuracy: 0.9991 - val_loss: 0.0749 - val_accuracy: 0.9808\n","Epoch 23/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0186 - accuracy: 0.9994 - val_loss: 0.0735 - val_accuracy: 0.9814\n","Epoch 24/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0178 - accuracy: 0.9995 - val_loss: 0.0748 - val_accuracy: 0.9808\n","Epoch 25/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0172 - accuracy: 0.9996 - val_loss: 0.0729 - val_accuracy: 0.9821\n","Epoch 26/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0166 - accuracy: 0.9997 - val_loss: 0.0740 - val_accuracy: 0.9815\n","Epoch 27/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0161 - accuracy: 0.9998 - val_loss: 0.0750 - val_accuracy: 0.9812\n","Epoch 28/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0157 - accuracy: 0.9998 - val_loss: 0.0744 - val_accuracy: 0.9811\n","Epoch 29/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0153 - accuracy: 0.9999 - val_loss: 0.0757 - val_accuracy: 0.9804\n","Epoch 30/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0150 - accuracy: 0.9999 - val_loss: 0.0748 - val_accuracy: 0.9817\n","Epoch 31/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0148 - accuracy: 0.9999 - val_loss: 0.0752 - val_accuracy: 0.9819\n","Epoch 32/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0145 - accuracy: 0.9999 - val_loss: 0.0748 - val_accuracy: 0.9814\n","Epoch 33/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0143 - accuracy: 0.9999 - val_loss: 0.0755 - val_accuracy: 0.9816\n","Epoch 34/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0141 - accuracy: 0.9999 - val_loss: 0.0752 - val_accuracy: 0.9813\n","Epoch 35/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0139 - accuracy: 1.0000 - val_loss: 0.0760 - val_accuracy: 0.9818\n","Epoch 36/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.0754 - val_accuracy: 0.9818\n","Epoch 37/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0136 - accuracy: 0.9999 - val_loss: 0.0751 - val_accuracy: 0.9819\n","Epoch 38/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.0761 - val_accuracy: 0.9817\n","Epoch 39/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.0768 - val_accuracy: 0.9821\n","Epoch 40/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0133 - accuracy: 1.0000 - val_loss: 0.0765 - val_accuracy: 0.9820\n","Regularization factor: 1e-05, Replicate: 3, Test loss: 0.07649735361337662, Test accuracy 0.9819999933242798\n","Epoch 1/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.5121 - accuracy: 0.8846 - val_loss: 0.3228 - val_accuracy: 0.9374\n","Epoch 2/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2883 - accuracy: 0.9449 - val_loss: 0.2565 - val_accuracy: 0.9535\n","Epoch 3/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2360 - accuracy: 0.9601 - val_loss: 0.2202 - val_accuracy: 0.9655\n","Epoch 4/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2069 - accuracy: 0.9689 - val_loss: 0.2024 - val_accuracy: 0.9704\n","Epoch 5/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1873 - accuracy: 0.9747 - val_loss: 0.1901 - val_accuracy: 0.9711\n","Epoch 6/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1723 - accuracy: 0.9781 - val_loss: 0.1814 - val_accuracy: 0.9734\n","Epoch 7/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1606 - accuracy: 0.9817 - val_loss: 0.1728 - val_accuracy: 0.9759\n","Epoch 8/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1510 - accuracy: 0.9841 - val_loss: 0.1706 - val_accuracy: 0.9765\n","Epoch 9/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1431 - accuracy: 0.9860 - val_loss: 0.1702 - val_accuracy: 0.9742\n","Epoch 10/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1360 - accuracy: 0.9885 - val_loss: 0.1611 - val_accuracy: 0.9780\n","Epoch 11/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1300 - accuracy: 0.9898 - val_loss: 0.1558 - val_accuracy: 0.9797\n","Epoch 12/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1246 - accuracy: 0.9917 - val_loss: 0.1535 - val_accuracy: 0.9804\n","Epoch 13/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1198 - accuracy: 0.9927 - val_loss: 0.1519 - val_accuracy: 0.9810\n","Epoch 14/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1155 - accuracy: 0.9937 - val_loss: 0.1499 - val_accuracy: 0.9810\n","Epoch 15/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1116 - accuracy: 0.9946 - val_loss: 0.1479 - val_accuracy: 0.9809\n","Epoch 16/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1081 - accuracy: 0.9953 - val_loss: 0.1465 - val_accuracy: 0.9816\n","Epoch 17/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1052 - accuracy: 0.9962 - val_loss: 0.1460 - val_accuracy: 0.9809\n","Epoch 18/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1019 - accuracy: 0.9968 - val_loss: 0.1432 - val_accuracy: 0.9815\n","Epoch 19/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0991 - accuracy: 0.9972 - val_loss: 0.1414 - val_accuracy: 0.9813\n","Epoch 20/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0969 - accuracy: 0.9976 - val_loss: 0.1420 - val_accuracy: 0.9823\n","Epoch 21/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0944 - accuracy: 0.9982 - val_loss: 0.1406 - val_accuracy: 0.9810\n","Epoch 22/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0921 - accuracy: 0.9985 - val_loss: 0.1378 - val_accuracy: 0.9816\n","Epoch 23/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0901 - accuracy: 0.9987 - val_loss: 0.1443 - val_accuracy: 0.9799\n","Epoch 24/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0884 - accuracy: 0.9990 - val_loss: 0.1364 - val_accuracy: 0.9814\n","Epoch 25/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0864 - accuracy: 0.9991 - val_loss: 0.1355 - val_accuracy: 0.9820\n","Epoch 26/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0849 - accuracy: 0.9994 - val_loss: 0.1396 - val_accuracy: 0.9804\n","Epoch 27/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0831 - accuracy: 0.9995 - val_loss: 0.1333 - val_accuracy: 0.9816\n","Epoch 28/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0817 - accuracy: 0.9995 - val_loss: 0.1313 - val_accuracy: 0.9818\n","Epoch 29/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0802 - accuracy: 0.9995 - val_loss: 0.1298 - val_accuracy: 0.9819\n","Epoch 30/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0787 - accuracy: 0.9997 - val_loss: 0.1277 - val_accuracy: 0.9825\n","Epoch 31/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0775 - accuracy: 0.9997 - val_loss: 0.1267 - val_accuracy: 0.9831\n","Epoch 32/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0762 - accuracy: 0.9997 - val_loss: 0.1277 - val_accuracy: 0.9827\n","Epoch 33/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0749 - accuracy: 0.9997 - val_loss: 0.1254 - val_accuracy: 0.9825\n","Epoch 34/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0738 - accuracy: 0.9998 - val_loss: 0.1245 - val_accuracy: 0.9825\n","Epoch 35/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0726 - accuracy: 0.9998 - val_loss: 0.1232 - val_accuracy: 0.9825\n","Epoch 36/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0714 - accuracy: 0.9998 - val_loss: 0.1217 - val_accuracy: 0.9828\n","Epoch 37/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0702 - accuracy: 0.9999 - val_loss: 0.1213 - val_accuracy: 0.9826\n","Epoch 38/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0692 - accuracy: 0.9999 - val_loss: 0.1205 - val_accuracy: 0.9820\n","Epoch 39/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0682 - accuracy: 0.9999 - val_loss: 0.1195 - val_accuracy: 0.9821\n","Epoch 40/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0672 - accuracy: 0.9999 - val_loss: 0.1171 - val_accuracy: 0.9827\n","Regularization factor: 0.0001, Replicate: 1, Test loss: 0.11713419109582901, Test accuracy 0.982699990272522\n","Epoch 1/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.5002 - accuracy: 0.8883 - val_loss: 0.3512 - val_accuracy: 0.9232\n","Epoch 2/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2901 - accuracy: 0.9451 - val_loss: 0.2590 - val_accuracy: 0.9532\n","Epoch 3/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2389 - accuracy: 0.9596 - val_loss: 0.2244 - val_accuracy: 0.9632\n","Epoch 4/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2085 - accuracy: 0.9686 - val_loss: 0.2047 - val_accuracy: 0.9672\n","Epoch 5/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1880 - accuracy: 0.9738 - val_loss: 0.1909 - val_accuracy: 0.9698\n","Epoch 6/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1725 - accuracy: 0.9781 - val_loss: 0.1794 - val_accuracy: 0.9754\n","Epoch 7/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1608 - accuracy: 0.9813 - val_loss: 0.1745 - val_accuracy: 0.9747\n","Epoch 8/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1507 - accuracy: 0.9841 - val_loss: 0.1689 - val_accuracy: 0.9768\n","Epoch 9/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1423 - accuracy: 0.9870 - val_loss: 0.1638 - val_accuracy: 0.9768\n","Epoch 10/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1348 - accuracy: 0.9893 - val_loss: 0.1649 - val_accuracy: 0.9759\n","Epoch 11/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1295 - accuracy: 0.9903 - val_loss: 0.1613 - val_accuracy: 0.9779\n","Epoch 12/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1237 - accuracy: 0.9918 - val_loss: 0.1551 - val_accuracy: 0.9791\n","Epoch 13/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1189 - accuracy: 0.9933 - val_loss: 0.1500 - val_accuracy: 0.9805\n","Epoch 14/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1148 - accuracy: 0.9940 - val_loss: 0.1573 - val_accuracy: 0.9777\n","Epoch 15/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1107 - accuracy: 0.9950 - val_loss: 0.1463 - val_accuracy: 0.9802\n","Epoch 16/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1072 - accuracy: 0.9960 - val_loss: 0.1464 - val_accuracy: 0.9802\n","Epoch 17/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1039 - accuracy: 0.9966 - val_loss: 0.1492 - val_accuracy: 0.9795\n","Epoch 18/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1012 - accuracy: 0.9970 - val_loss: 0.1445 - val_accuracy: 0.9806\n","Epoch 19/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0985 - accuracy: 0.9977 - val_loss: 0.1464 - val_accuracy: 0.9809\n","Epoch 20/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0961 - accuracy: 0.9978 - val_loss: 0.1405 - val_accuracy: 0.9801\n","Epoch 21/40\n","469/469 [==============================] - 4s 8ms/step - loss: 0.0937 - accuracy: 0.9984 - val_loss: 0.1370 - val_accuracy: 0.9826\n","Epoch 22/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0914 - accuracy: 0.9988 - val_loss: 0.1377 - val_accuracy: 0.9817\n","Epoch 23/40\n","469/469 [==============================] - 4s 8ms/step - loss: 0.0895 - accuracy: 0.9988 - val_loss: 0.1373 - val_accuracy: 0.9807\n","Epoch 24/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0877 - accuracy: 0.9992 - val_loss: 0.1343 - val_accuracy: 0.9819\n","Epoch 25/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0861 - accuracy: 0.9993 - val_loss: 0.1351 - val_accuracy: 0.9815\n","Epoch 26/40\n","469/469 [==============================] - 4s 8ms/step - loss: 0.0843 - accuracy: 0.9994 - val_loss: 0.1326 - val_accuracy: 0.9817\n","Epoch 27/40\n","469/469 [==============================] - 4s 8ms/step - loss: 0.0828 - accuracy: 0.9995 - val_loss: 0.1317 - val_accuracy: 0.9819\n","Epoch 28/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0812 - accuracy: 0.9996 - val_loss: 0.1293 - val_accuracy: 0.9822\n","Epoch 29/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0798 - accuracy: 0.9996 - val_loss: 0.1301 - val_accuracy: 0.9821\n","Epoch 30/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0783 - accuracy: 0.9997 - val_loss: 0.1297 - val_accuracy: 0.9813\n","Epoch 31/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0770 - accuracy: 0.9998 - val_loss: 0.1361 - val_accuracy: 0.9803\n","Epoch 32/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0759 - accuracy: 0.9998 - val_loss: 0.1269 - val_accuracy: 0.9816\n","Epoch 33/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0746 - accuracy: 0.9999 - val_loss: 0.1253 - val_accuracy: 0.9821\n","Epoch 34/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0734 - accuracy: 0.9998 - val_loss: 0.1254 - val_accuracy: 0.9815\n","Epoch 35/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0722 - accuracy: 0.9998 - val_loss: 0.1231 - val_accuracy: 0.9818\n","Epoch 36/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0710 - accuracy: 0.9999 - val_loss: 0.1217 - val_accuracy: 0.9821\n","Epoch 37/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0700 - accuracy: 0.9999 - val_loss: 0.1223 - val_accuracy: 0.9814\n","Epoch 38/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0689 - accuracy: 0.9999 - val_loss: 0.1207 - val_accuracy: 0.9822\n","Epoch 39/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0679 - accuracy: 0.9999 - val_loss: 0.1205 - val_accuracy: 0.9825\n","Epoch 40/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0669 - accuracy: 0.9999 - val_loss: 0.1193 - val_accuracy: 0.9822\n","Regularization factor: 0.0001, Replicate: 2, Test loss: 0.11932330578565598, Test accuracy 0.982200026512146\n","Epoch 1/40\n","469/469 [==============================] - 4s 7ms/step - loss: 0.5057 - accuracy: 0.8856 - val_loss: 0.3154 - val_accuracy: 0.9366\n","Epoch 2/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2914 - accuracy: 0.9442 - val_loss: 0.2645 - val_accuracy: 0.9507\n","Epoch 3/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2390 - accuracy: 0.9596 - val_loss: 0.2296 - val_accuracy: 0.9608\n","Epoch 4/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2083 - accuracy: 0.9682 - val_loss: 0.2016 - val_accuracy: 0.9699\n","Epoch 5/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1874 - accuracy: 0.9747 - val_loss: 0.1862 - val_accuracy: 0.9722\n","Epoch 6/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1719 - accuracy: 0.9785 - val_loss: 0.1886 - val_accuracy: 0.9698\n","Epoch 7/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1595 - accuracy: 0.9818 - val_loss: 0.1718 - val_accuracy: 0.9772\n","Epoch 8/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1493 - accuracy: 0.9852 - val_loss: 0.1669 - val_accuracy: 0.9784\n","Epoch 9/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1414 - accuracy: 0.9870 - val_loss: 0.1629 - val_accuracy: 0.9783\n","Epoch 10/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1342 - accuracy: 0.9890 - val_loss: 0.1583 - val_accuracy: 0.9798\n","Epoch 11/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1283 - accuracy: 0.9908 - val_loss: 0.1616 - val_accuracy: 0.9786\n","Epoch 12/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1225 - accuracy: 0.9923 - val_loss: 0.1542 - val_accuracy: 0.9798\n","Epoch 13/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1181 - accuracy: 0.9936 - val_loss: 0.1522 - val_accuracy: 0.9814\n","Epoch 14/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1137 - accuracy: 0.9944 - val_loss: 0.1516 - val_accuracy: 0.9805\n","Epoch 15/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1098 - accuracy: 0.9955 - val_loss: 0.1473 - val_accuracy: 0.9811\n","Epoch 16/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1066 - accuracy: 0.9961 - val_loss: 0.1454 - val_accuracy: 0.9821\n","Epoch 17/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1033 - accuracy: 0.9967 - val_loss: 0.1452 - val_accuracy: 0.9807\n","Epoch 18/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1006 - accuracy: 0.9971 - val_loss: 0.1477 - val_accuracy: 0.9797\n","Epoch 19/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0978 - accuracy: 0.9979 - val_loss: 0.1400 - val_accuracy: 0.9821\n","Epoch 20/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0955 - accuracy: 0.9981 - val_loss: 0.1394 - val_accuracy: 0.9819\n","Epoch 21/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0933 - accuracy: 0.9984 - val_loss: 0.1386 - val_accuracy: 0.9813\n","Epoch 22/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0911 - accuracy: 0.9990 - val_loss: 0.1422 - val_accuracy: 0.9807\n","Epoch 23/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0891 - accuracy: 0.9991 - val_loss: 0.1362 - val_accuracy: 0.9820\n","Epoch 24/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0874 - accuracy: 0.9991 - val_loss: 0.1345 - val_accuracy: 0.9816\n","Epoch 25/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0856 - accuracy: 0.9992 - val_loss: 0.1340 - val_accuracy: 0.9826\n","Epoch 26/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0840 - accuracy: 0.9994 - val_loss: 0.1315 - val_accuracy: 0.9825\n","Epoch 27/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0825 - accuracy: 0.9995 - val_loss: 0.1324 - val_accuracy: 0.9825\n","Epoch 28/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0810 - accuracy: 0.9996 - val_loss: 0.1301 - val_accuracy: 0.9824\n","Epoch 29/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0794 - accuracy: 0.9997 - val_loss: 0.1289 - val_accuracy: 0.9828\n","Epoch 30/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0782 - accuracy: 0.9997 - val_loss: 0.1298 - val_accuracy: 0.9818\n","Epoch 31/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0769 - accuracy: 0.9998 - val_loss: 0.1270 - val_accuracy: 0.9822\n","Epoch 32/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0756 - accuracy: 0.9998 - val_loss: 0.1266 - val_accuracy: 0.9825\n","Epoch 33/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0744 - accuracy: 0.9998 - val_loss: 0.1262 - val_accuracy: 0.9815\n","Epoch 34/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0732 - accuracy: 0.9998 - val_loss: 0.1256 - val_accuracy: 0.9819\n","Epoch 35/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0720 - accuracy: 0.9999 - val_loss: 0.1272 - val_accuracy: 0.9814\n","Epoch 36/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0710 - accuracy: 0.9999 - val_loss: 0.1218 - val_accuracy: 0.9830\n","Epoch 37/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0699 - accuracy: 0.9999 - val_loss: 0.1211 - val_accuracy: 0.9838\n","Epoch 38/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0687 - accuracy: 0.9999 - val_loss: 0.1203 - val_accuracy: 0.9826\n","Epoch 39/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0677 - accuracy: 0.9999 - val_loss: 0.1207 - val_accuracy: 0.9827\n","Epoch 40/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0667 - accuracy: 1.0000 - val_loss: 0.1191 - val_accuracy: 0.9820\n","Regularization factor: 0.0001, Replicate: 3, Test loss: 0.11907196044921875, Test accuracy 0.9819999933242798\n","Epoch 1/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.8868 - accuracy: 0.8871 - val_loss: 0.7206 - val_accuracy: 0.9246\n","Epoch 2/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.6416 - accuracy: 0.9441 - val_loss: 0.5970 - val_accuracy: 0.9500\n","Epoch 3/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.5590 - accuracy: 0.9584 - val_loss: 0.5481 - val_accuracy: 0.9543\n","Epoch 4/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.4995 - accuracy: 0.9671 - val_loss: 0.4795 - val_accuracy: 0.9660\n","Epoch 5/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.4519 - accuracy: 0.9718 - val_loss: 0.4648 - val_accuracy: 0.9626\n","Epoch 6/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.4122 - accuracy: 0.9762 - val_loss: 0.4161 - val_accuracy: 0.9703\n","Epoch 7/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.3779 - accuracy: 0.9795 - val_loss: 0.3824 - val_accuracy: 0.9729\n","Epoch 8/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.3480 - accuracy: 0.9824 - val_loss: 0.3606 - val_accuracy: 0.9739\n","Epoch 9/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.3209 - accuracy: 0.9841 - val_loss: 0.3340 - val_accuracy: 0.9764\n","Epoch 10/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2974 - accuracy: 0.9859 - val_loss: 0.3086 - val_accuracy: 0.9778\n","Epoch 11/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2760 - accuracy: 0.9871 - val_loss: 0.2879 - val_accuracy: 0.9794\n","Epoch 12/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2564 - accuracy: 0.9884 - val_loss: 0.2712 - val_accuracy: 0.9797\n","Epoch 13/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2388 - accuracy: 0.9896 - val_loss: 0.2558 - val_accuracy: 0.9798\n","Epoch 14/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2228 - accuracy: 0.9901 - val_loss: 0.2450 - val_accuracy: 0.9803\n","Epoch 15/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2089 - accuracy: 0.9910 - val_loss: 0.2293 - val_accuracy: 0.9822\n","Epoch 16/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1960 - accuracy: 0.9917 - val_loss: 0.2190 - val_accuracy: 0.9805\n","Epoch 17/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1839 - accuracy: 0.9923 - val_loss: 0.2140 - val_accuracy: 0.9789\n","Epoch 18/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1731 - accuracy: 0.9930 - val_loss: 0.2007 - val_accuracy: 0.9806\n","Epoch 19/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1637 - accuracy: 0.9933 - val_loss: 0.1948 - val_accuracy: 0.9799\n","Epoch 20/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1543 - accuracy: 0.9942 - val_loss: 0.1816 - val_accuracy: 0.9822\n","Epoch 21/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1461 - accuracy: 0.9946 - val_loss: 0.1749 - val_accuracy: 0.9813\n","Epoch 22/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1384 - accuracy: 0.9945 - val_loss: 0.1696 - val_accuracy: 0.9824\n","Epoch 23/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1319 - accuracy: 0.9952 - val_loss: 0.1618 - val_accuracy: 0.9820\n","Epoch 24/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1248 - accuracy: 0.9958 - val_loss: 0.1548 - val_accuracy: 0.9820\n","Epoch 25/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1194 - accuracy: 0.9956 - val_loss: 0.1519 - val_accuracy: 0.9816\n","Epoch 26/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1142 - accuracy: 0.9958 - val_loss: 0.1452 - val_accuracy: 0.9833\n","Epoch 27/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1094 - accuracy: 0.9959 - val_loss: 0.1451 - val_accuracy: 0.9815\n","Epoch 28/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1046 - accuracy: 0.9963 - val_loss: 0.1425 - val_accuracy: 0.9813\n","Epoch 29/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1005 - accuracy: 0.9967 - val_loss: 0.1343 - val_accuracy: 0.9819\n","Epoch 30/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0968 - accuracy: 0.9967 - val_loss: 0.1332 - val_accuracy: 0.9811\n","Epoch 31/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0931 - accuracy: 0.9971 - val_loss: 0.1329 - val_accuracy: 0.9801\n","Epoch 32/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0900 - accuracy: 0.9971 - val_loss: 0.1368 - val_accuracy: 0.9782\n","Epoch 33/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0871 - accuracy: 0.9973 - val_loss: 0.1281 - val_accuracy: 0.9819\n","Epoch 34/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0842 - accuracy: 0.9977 - val_loss: 0.1225 - val_accuracy: 0.9824\n","Epoch 35/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0824 - accuracy: 0.9972 - val_loss: 0.1202 - val_accuracy: 0.9817\n","Epoch 36/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0798 - accuracy: 0.9974 - val_loss: 0.1177 - val_accuracy: 0.9824\n","Epoch 37/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0781 - accuracy: 0.9972 - val_loss: 0.1138 - val_accuracy: 0.9817\n","Epoch 38/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0757 - accuracy: 0.9976 - val_loss: 0.1136 - val_accuracy: 0.9827\n","Epoch 39/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0739 - accuracy: 0.9976 - val_loss: 0.1120 - val_accuracy: 0.9825\n","Epoch 40/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0720 - accuracy: 0.9978 - val_loss: 0.1141 - val_accuracy: 0.9808\n","Regularization factor: 0.0005, Replicate: 1, Test loss: 0.11408642679452896, Test accuracy 0.9807999730110168\n","Epoch 1/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.8911 - accuracy: 0.8858 - val_loss: 0.6919 - val_accuracy: 0.9328\n","Epoch 2/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.6401 - accuracy: 0.9444 - val_loss: 0.5884 - val_accuracy: 0.9523\n","Epoch 3/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.5566 - accuracy: 0.9587 - val_loss: 0.5336 - val_accuracy: 0.9590\n","Epoch 4/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.4982 - accuracy: 0.9669 - val_loss: 0.4798 - val_accuracy: 0.9671\n","Epoch 5/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.4516 - accuracy: 0.9722 - val_loss: 0.4357 - val_accuracy: 0.9702\n","Epoch 6/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.4115 - accuracy: 0.9766 - val_loss: 0.4100 - val_accuracy: 0.9703\n","Epoch 7/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.3779 - accuracy: 0.9796 - val_loss: 0.3836 - val_accuracy: 0.9717\n","Epoch 8/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.3478 - accuracy: 0.9818 - val_loss: 0.3545 - val_accuracy: 0.9747\n","Epoch 9/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.3211 - accuracy: 0.9839 - val_loss: 0.3294 - val_accuracy: 0.9760\n","Epoch 10/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2972 - accuracy: 0.9858 - val_loss: 0.3028 - val_accuracy: 0.9791\n","Epoch 11/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2754 - accuracy: 0.9874 - val_loss: 0.2865 - val_accuracy: 0.9792\n","Epoch 12/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2568 - accuracy: 0.9882 - val_loss: 0.2734 - val_accuracy: 0.9777\n","Epoch 13/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2392 - accuracy: 0.9896 - val_loss: 0.2548 - val_accuracy: 0.9789\n","Epoch 14/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2234 - accuracy: 0.9898 - val_loss: 0.2448 - val_accuracy: 0.9784\n","Epoch 15/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2087 - accuracy: 0.9916 - val_loss: 0.2351 - val_accuracy: 0.9782\n","Epoch 16/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1961 - accuracy: 0.9916 - val_loss: 0.2178 - val_accuracy: 0.9798\n","Epoch 17/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1842 - accuracy: 0.9924 - val_loss: 0.2055 - val_accuracy: 0.9825\n","Epoch 18/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1735 - accuracy: 0.9928 - val_loss: 0.1978 - val_accuracy: 0.9821\n","Epoch 19/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1639 - accuracy: 0.9933 - val_loss: 0.1895 - val_accuracy: 0.9814\n","Epoch 20/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1544 - accuracy: 0.9942 - val_loss: 0.1777 - val_accuracy: 0.9827\n","Epoch 21/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1457 - accuracy: 0.9948 - val_loss: 0.1763 - val_accuracy: 0.9801\n","Epoch 22/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1387 - accuracy: 0.9949 - val_loss: 0.1676 - val_accuracy: 0.9818\n","Epoch 23/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1314 - accuracy: 0.9952 - val_loss: 0.1603 - val_accuracy: 0.9826\n","Epoch 24/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1254 - accuracy: 0.9956 - val_loss: 0.1557 - val_accuracy: 0.9818\n","Epoch 25/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1199 - accuracy: 0.9957 - val_loss: 0.1513 - val_accuracy: 0.9821\n","Epoch 26/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1141 - accuracy: 0.9960 - val_loss: 0.1436 - val_accuracy: 0.9812\n","Epoch 27/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1093 - accuracy: 0.9962 - val_loss: 0.1417 - val_accuracy: 0.9815\n","Epoch 28/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1052 - accuracy: 0.9964 - val_loss: 0.1368 - val_accuracy: 0.9828\n","Epoch 29/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1010 - accuracy: 0.9965 - val_loss: 0.1362 - val_accuracy: 0.9802\n","Epoch 30/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0971 - accuracy: 0.9968 - val_loss: 0.1326 - val_accuracy: 0.9821\n","Epoch 31/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0935 - accuracy: 0.9967 - val_loss: 0.1268 - val_accuracy: 0.9815\n","Epoch 32/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0904 - accuracy: 0.9968 - val_loss: 0.1279 - val_accuracy: 0.9816\n","Epoch 33/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0875 - accuracy: 0.9972 - val_loss: 0.1260 - val_accuracy: 0.9815\n","Epoch 34/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0846 - accuracy: 0.9974 - val_loss: 0.1204 - val_accuracy: 0.9827\n","Epoch 35/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0821 - accuracy: 0.9976 - val_loss: 0.1150 - val_accuracy: 0.9829\n","Epoch 36/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0803 - accuracy: 0.9969 - val_loss: 0.1160 - val_accuracy: 0.9814\n","Epoch 37/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0775 - accuracy: 0.9974 - val_loss: 0.1140 - val_accuracy: 0.9832\n","Epoch 38/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0760 - accuracy: 0.9977 - val_loss: 0.1157 - val_accuracy: 0.9810\n","Epoch 39/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0738 - accuracy: 0.9976 - val_loss: 0.1112 - val_accuracy: 0.9829\n","Epoch 40/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0717 - accuracy: 0.9979 - val_loss: 0.1076 - val_accuracy: 0.9831\n","Regularization factor: 0.0005, Replicate: 2, Test loss: 0.10755469650030136, Test accuracy 0.9830999970436096\n","Epoch 1/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.8864 - accuracy: 0.8870 - val_loss: 0.6959 - val_accuracy: 0.9333\n","Epoch 2/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.6396 - accuracy: 0.9437 - val_loss: 0.5850 - val_accuracy: 0.9541\n","Epoch 3/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.5575 - accuracy: 0.9583 - val_loss: 0.5254 - val_accuracy: 0.9622\n","Epoch 4/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.4985 - accuracy: 0.9671 - val_loss: 0.4840 - val_accuracy: 0.9652\n","Epoch 5/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.4515 - accuracy: 0.9725 - val_loss: 0.4453 - val_accuracy: 0.9681\n","Epoch 6/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.4122 - accuracy: 0.9768 - val_loss: 0.4078 - val_accuracy: 0.9717\n","Epoch 7/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.3773 - accuracy: 0.9800 - val_loss: 0.3750 - val_accuracy: 0.9752\n","Epoch 8/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.3474 - accuracy: 0.9826 - val_loss: 0.3593 - val_accuracy: 0.9733\n","Epoch 9/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.3207 - accuracy: 0.9844 - val_loss: 0.3293 - val_accuracy: 0.9770\n","Epoch 10/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2967 - accuracy: 0.9861 - val_loss: 0.3089 - val_accuracy: 0.9779\n","Epoch 11/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2761 - accuracy: 0.9871 - val_loss: 0.2909 - val_accuracy: 0.9776\n","Epoch 12/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2566 - accuracy: 0.9881 - val_loss: 0.2821 - val_accuracy: 0.9765\n","Epoch 13/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2386 - accuracy: 0.9895 - val_loss: 0.2650 - val_accuracy: 0.9772\n","Epoch 14/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2228 - accuracy: 0.9909 - val_loss: 0.2424 - val_accuracy: 0.9795\n","Epoch 15/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2086 - accuracy: 0.9911 - val_loss: 0.2331 - val_accuracy: 0.9795\n","Epoch 16/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1955 - accuracy: 0.9918 - val_loss: 0.2216 - val_accuracy: 0.9794\n","Epoch 17/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1843 - accuracy: 0.9926 - val_loss: 0.2088 - val_accuracy: 0.9803\n","Epoch 18/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1731 - accuracy: 0.9931 - val_loss: 0.2043 - val_accuracy: 0.9785\n","Epoch 19/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1637 - accuracy: 0.9930 - val_loss: 0.1938 - val_accuracy: 0.9816\n","Epoch 20/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1540 - accuracy: 0.9941 - val_loss: 0.1834 - val_accuracy: 0.9811\n","Epoch 21/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1457 - accuracy: 0.9947 - val_loss: 0.1742 - val_accuracy: 0.9811\n","Epoch 22/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1385 - accuracy: 0.9948 - val_loss: 0.1705 - val_accuracy: 0.9810\n","Epoch 23/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1315 - accuracy: 0.9949 - val_loss: 0.1648 - val_accuracy: 0.9811\n","Epoch 24/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1253 - accuracy: 0.9953 - val_loss: 0.1576 - val_accuracy: 0.9804\n","Epoch 25/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1196 - accuracy: 0.9957 - val_loss: 0.1521 - val_accuracy: 0.9822\n","Epoch 26/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1142 - accuracy: 0.9960 - val_loss: 0.1522 - val_accuracy: 0.9803\n","Epoch 27/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1095 - accuracy: 0.9961 - val_loss: 0.1475 - val_accuracy: 0.9797\n","Epoch 28/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1048 - accuracy: 0.9961 - val_loss: 0.1380 - val_accuracy: 0.9811\n","Epoch 29/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1007 - accuracy: 0.9967 - val_loss: 0.1338 - val_accuracy: 0.9817\n","Epoch 30/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0968 - accuracy: 0.9969 - val_loss: 0.1330 - val_accuracy: 0.9812\n","Epoch 31/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0939 - accuracy: 0.9968 - val_loss: 0.1326 - val_accuracy: 0.9805\n","Epoch 32/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0904 - accuracy: 0.9970 - val_loss: 0.1275 - val_accuracy: 0.9811\n","Epoch 33/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0875 - accuracy: 0.9970 - val_loss: 0.1275 - val_accuracy: 0.9813\n","Epoch 34/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0844 - accuracy: 0.9973 - val_loss: 0.1204 - val_accuracy: 0.9827\n","Epoch 35/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0819 - accuracy: 0.9974 - val_loss: 0.1225 - val_accuracy: 0.9814\n","Epoch 36/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0800 - accuracy: 0.9976 - val_loss: 0.1174 - val_accuracy: 0.9812\n","Epoch 37/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0776 - accuracy: 0.9975 - val_loss: 0.1141 - val_accuracy: 0.9827\n","Epoch 38/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0757 - accuracy: 0.9977 - val_loss: 0.1155 - val_accuracy: 0.9806\n","Epoch 39/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0737 - accuracy: 0.9978 - val_loss: 0.1103 - val_accuracy: 0.9825\n","Epoch 40/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0722 - accuracy: 0.9977 - val_loss: 0.1103 - val_accuracy: 0.9819\n","Regularization factor: 0.0005, Replicate: 3, Test loss: 0.1102679967880249, Test accuracy 0.9818999767303467\n","Epoch 1/40\n","469/469 [==============================] - 3s 6ms/step - loss: 1.3239 - accuracy: 0.8873 - val_loss: 1.0683 - val_accuracy: 0.9338\n","Epoch 2/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.9760 - accuracy: 0.9436 - val_loss: 0.8830 - val_accuracy: 0.9536\n","Epoch 3/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.8102 - accuracy: 0.9575 - val_loss: 0.7421 - val_accuracy: 0.9616\n","Epoch 4/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.6850 - accuracy: 0.9655 - val_loss: 0.6321 - val_accuracy: 0.9669\n","Epoch 5/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.5843 - accuracy: 0.9706 - val_loss: 0.5493 - val_accuracy: 0.9674\n","Epoch 6/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.5036 - accuracy: 0.9747 - val_loss: 0.4917 - val_accuracy: 0.9658\n","Epoch 7/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.4365 - accuracy: 0.9775 - val_loss: 0.4247 - val_accuracy: 0.9716\n","Epoch 8/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.3816 - accuracy: 0.9793 - val_loss: 0.3707 - val_accuracy: 0.9751\n","Epoch 9/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.3365 - accuracy: 0.9812 - val_loss: 0.3269 - val_accuracy: 0.9772\n","Epoch 10/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2981 - accuracy: 0.9830 - val_loss: 0.3009 - val_accuracy: 0.9752\n","Epoch 11/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2664 - accuracy: 0.9838 - val_loss: 0.2736 - val_accuracy: 0.9753\n","Epoch 12/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2405 - accuracy: 0.9846 - val_loss: 0.2507 - val_accuracy: 0.9762\n","Epoch 13/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2184 - accuracy: 0.9856 - val_loss: 0.2287 - val_accuracy: 0.9775\n","Epoch 14/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1993 - accuracy: 0.9867 - val_loss: 0.2123 - val_accuracy: 0.9781\n","Epoch 15/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1840 - accuracy: 0.9872 - val_loss: 0.1954 - val_accuracy: 0.9787\n","Epoch 16/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1708 - accuracy: 0.9877 - val_loss: 0.1869 - val_accuracy: 0.9773\n","Epoch 17/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1596 - accuracy: 0.9881 - val_loss: 0.1838 - val_accuracy: 0.9760\n","Epoch 18/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1504 - accuracy: 0.9884 - val_loss: 0.1830 - val_accuracy: 0.9739\n","Epoch 19/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1425 - accuracy: 0.9891 - val_loss: 0.1665 - val_accuracy: 0.9778\n","Epoch 20/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1356 - accuracy: 0.9890 - val_loss: 0.1590 - val_accuracy: 0.9781\n","Epoch 21/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1298 - accuracy: 0.9895 - val_loss: 0.1537 - val_accuracy: 0.9788\n","Epoch 22/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1244 - accuracy: 0.9901 - val_loss: 0.1529 - val_accuracy: 0.9793\n","Epoch 23/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1198 - accuracy: 0.9906 - val_loss: 0.1404 - val_accuracy: 0.9815\n","Epoch 24/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1163 - accuracy: 0.9906 - val_loss: 0.1403 - val_accuracy: 0.9793\n","Epoch 25/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1134 - accuracy: 0.9905 - val_loss: 0.1375 - val_accuracy: 0.9809\n","Epoch 26/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1100 - accuracy: 0.9913 - val_loss: 0.1455 - val_accuracy: 0.9745\n","Epoch 27/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1074 - accuracy: 0.9915 - val_loss: 0.1315 - val_accuracy: 0.9799\n","Epoch 28/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1051 - accuracy: 0.9915 - val_loss: 0.1321 - val_accuracy: 0.9793\n","Epoch 29/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1032 - accuracy: 0.9916 - val_loss: 0.1239 - val_accuracy: 0.9823\n","Epoch 30/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1015 - accuracy: 0.9920 - val_loss: 0.1255 - val_accuracy: 0.9803\n","Epoch 31/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0996 - accuracy: 0.9923 - val_loss: 0.1268 - val_accuracy: 0.9794\n","Epoch 32/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0985 - accuracy: 0.9922 - val_loss: 0.1239 - val_accuracy: 0.9812\n","Epoch 33/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0963 - accuracy: 0.9926 - val_loss: 0.1303 - val_accuracy: 0.9788\n","Epoch 34/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0959 - accuracy: 0.9925 - val_loss: 0.1200 - val_accuracy: 0.9807\n","Epoch 35/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0946 - accuracy: 0.9926 - val_loss: 0.1218 - val_accuracy: 0.9820\n","Epoch 36/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0932 - accuracy: 0.9928 - val_loss: 0.1173 - val_accuracy: 0.9816\n","Epoch 37/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0924 - accuracy: 0.9928 - val_loss: 0.1382 - val_accuracy: 0.9757\n","Epoch 38/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0914 - accuracy: 0.9932 - val_loss: 0.1621 - val_accuracy: 0.9648\n","Epoch 39/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0906 - accuracy: 0.9931 - val_loss: 0.1191 - val_accuracy: 0.9807\n","Epoch 40/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0893 - accuracy: 0.9933 - val_loss: 0.1229 - val_accuracy: 0.9783\n","Regularization factor: 0.001, Replicate: 1, Test loss: 0.12285088002681732, Test accuracy 0.9782999753952026\n","Epoch 1/40\n","469/469 [==============================] - 4s 7ms/step - loss: 1.3270 - accuracy: 0.8879 - val_loss: 1.0723 - val_accuracy: 0.9356\n","Epoch 2/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.9800 - accuracy: 0.9430 - val_loss: 0.8781 - val_accuracy: 0.9536\n","Epoch 3/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.8139 - accuracy: 0.9566 - val_loss: 0.7372 - val_accuracy: 0.9644\n","Epoch 4/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.6867 - accuracy: 0.9651 - val_loss: 0.6399 - val_accuracy: 0.9622\n","Epoch 5/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.5860 - accuracy: 0.9708 - val_loss: 0.5712 - val_accuracy: 0.9600\n","Epoch 6/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.5050 - accuracy: 0.9745 - val_loss: 0.5017 - val_accuracy: 0.9631\n","Epoch 7/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.4379 - accuracy: 0.9769 - val_loss: 0.4148 - val_accuracy: 0.9740\n","Epoch 8/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.3828 - accuracy: 0.9796 - val_loss: 0.3692 - val_accuracy: 0.9736\n","Epoch 9/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.3368 - accuracy: 0.9813 - val_loss: 0.3272 - val_accuracy: 0.9769\n","Epoch 10/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2986 - accuracy: 0.9823 - val_loss: 0.3002 - val_accuracy: 0.9748\n","Epoch 11/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2673 - accuracy: 0.9839 - val_loss: 0.2799 - val_accuracy: 0.9734\n","Epoch 12/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2409 - accuracy: 0.9844 - val_loss: 0.2448 - val_accuracy: 0.9789\n","Epoch 13/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2193 - accuracy: 0.9846 - val_loss: 0.2241 - val_accuracy: 0.9771\n","Epoch 14/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1999 - accuracy: 0.9859 - val_loss: 0.2064 - val_accuracy: 0.9802\n","Epoch 15/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1844 - accuracy: 0.9867 - val_loss: 0.2101 - val_accuracy: 0.9749\n","Epoch 16/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1710 - accuracy: 0.9874 - val_loss: 0.1888 - val_accuracy: 0.9786\n","Epoch 17/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1595 - accuracy: 0.9884 - val_loss: 0.1772 - val_accuracy: 0.9794\n","Epoch 18/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1502 - accuracy: 0.9885 - val_loss: 0.1686 - val_accuracy: 0.9789\n","Epoch 19/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1425 - accuracy: 0.9890 - val_loss: 0.1666 - val_accuracy: 0.9779\n","Epoch 20/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1356 - accuracy: 0.9891 - val_loss: 0.1584 - val_accuracy: 0.9791\n","Epoch 21/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1295 - accuracy: 0.9897 - val_loss: 0.1556 - val_accuracy: 0.9801\n","Epoch 22/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1243 - accuracy: 0.9900 - val_loss: 0.1457 - val_accuracy: 0.9802\n","Epoch 23/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1205 - accuracy: 0.9902 - val_loss: 0.1419 - val_accuracy: 0.9805\n","Epoch 24/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1156 - accuracy: 0.9912 - val_loss: 0.1406 - val_accuracy: 0.9797\n","Epoch 25/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1131 - accuracy: 0.9905 - val_loss: 0.1346 - val_accuracy: 0.9812\n","Epoch 26/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1099 - accuracy: 0.9912 - val_loss: 0.1333 - val_accuracy: 0.9806\n","Epoch 27/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1071 - accuracy: 0.9917 - val_loss: 0.1341 - val_accuracy: 0.9792\n","Epoch 28/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1049 - accuracy: 0.9919 - val_loss: 0.1307 - val_accuracy: 0.9802\n","Epoch 29/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1039 - accuracy: 0.9914 - val_loss: 0.1272 - val_accuracy: 0.9808\n","Epoch 30/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1007 - accuracy: 0.9923 - val_loss: 0.1304 - val_accuracy: 0.9792\n","Epoch 31/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0998 - accuracy: 0.9922 - val_loss: 0.1234 - val_accuracy: 0.9824\n","Epoch 32/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0981 - accuracy: 0.9920 - val_loss: 0.1226 - val_accuracy: 0.9826\n","Epoch 33/40\n","469/469 [==============================] - 3s 7ms/step - loss: 0.0966 - accuracy: 0.9928 - val_loss: 0.1229 - val_accuracy: 0.9806\n","Epoch 34/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0955 - accuracy: 0.9928 - val_loss: 0.1253 - val_accuracy: 0.9803\n","Epoch 35/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0946 - accuracy: 0.9924 - val_loss: 0.1182 - val_accuracy: 0.9821\n","Epoch 36/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0933 - accuracy: 0.9932 - val_loss: 0.1194 - val_accuracy: 0.9828\n","Epoch 37/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0930 - accuracy: 0.9930 - val_loss: 0.1218 - val_accuracy: 0.9802\n","Epoch 38/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0909 - accuracy: 0.9936 - val_loss: 0.1202 - val_accuracy: 0.9808\n","Epoch 39/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0909 - accuracy: 0.9928 - val_loss: 0.1169 - val_accuracy: 0.9814\n","Epoch 40/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1115 - accuracy: 0.9875 - val_loss: 0.1267 - val_accuracy: 0.9785\n","Regularization factor: 0.001, Replicate: 2, Test loss: 0.12666599452495575, Test accuracy 0.9785000085830688\n","Epoch 1/40\n","469/469 [==============================] - 4s 7ms/step - loss: 1.3343 - accuracy: 0.8844 - val_loss: 1.0838 - val_accuracy: 0.9316\n","Epoch 2/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.9875 - accuracy: 0.9414 - val_loss: 0.8894 - val_accuracy: 0.9499\n","Epoch 3/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.8194 - accuracy: 0.9557 - val_loss: 0.7498 - val_accuracy: 0.9594\n","Epoch 4/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.6908 - accuracy: 0.9642 - val_loss: 0.6405 - val_accuracy: 0.9641\n","Epoch 5/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.5892 - accuracy: 0.9696 - val_loss: 0.5501 - val_accuracy: 0.9681\n","Epoch 6/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.5077 - accuracy: 0.9728 - val_loss: 0.4788 - val_accuracy: 0.9719\n","Epoch 7/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.4402 - accuracy: 0.9768 - val_loss: 0.4162 - val_accuracy: 0.9733\n","Epoch 8/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.3847 - accuracy: 0.9788 - val_loss: 0.3705 - val_accuracy: 0.9752\n","Epoch 9/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.3380 - accuracy: 0.9811 - val_loss: 0.3322 - val_accuracy: 0.9752\n","Epoch 10/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2996 - accuracy: 0.9826 - val_loss: 0.2995 - val_accuracy: 0.9751\n","Epoch 11/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2679 - accuracy: 0.9834 - val_loss: 0.2702 - val_accuracy: 0.9768\n","Epoch 12/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2422 - accuracy: 0.9845 - val_loss: 0.2459 - val_accuracy: 0.9777\n","Epoch 13/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2192 - accuracy: 0.9858 - val_loss: 0.2252 - val_accuracy: 0.9778\n","Epoch 14/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.2003 - accuracy: 0.9864 - val_loss: 0.2117 - val_accuracy: 0.9781\n","Epoch 15/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1848 - accuracy: 0.9869 - val_loss: 0.1985 - val_accuracy: 0.9781\n","Epoch 16/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1723 - accuracy: 0.9871 - val_loss: 0.1864 - val_accuracy: 0.9790\n","Epoch 17/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1605 - accuracy: 0.9882 - val_loss: 0.1769 - val_accuracy: 0.9787\n","Epoch 18/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1510 - accuracy: 0.9887 - val_loss: 0.1658 - val_accuracy: 0.9794\n","Epoch 19/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1425 - accuracy: 0.9890 - val_loss: 0.1588 - val_accuracy: 0.9805\n","Epoch 20/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1360 - accuracy: 0.9893 - val_loss: 0.1613 - val_accuracy: 0.9786\n","Epoch 21/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1304 - accuracy: 0.9897 - val_loss: 0.1508 - val_accuracy: 0.9800\n","Epoch 22/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1245 - accuracy: 0.9901 - val_loss: 0.1496 - val_accuracy: 0.9792\n","Epoch 23/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1207 - accuracy: 0.9902 - val_loss: 0.1417 - val_accuracy: 0.9812\n","Epoch 24/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1166 - accuracy: 0.9910 - val_loss: 0.1409 - val_accuracy: 0.9801\n","Epoch 25/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1131 - accuracy: 0.9910 - val_loss: 0.1381 - val_accuracy: 0.9804\n","Epoch 26/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1101 - accuracy: 0.9916 - val_loss: 0.1350 - val_accuracy: 0.9800\n","Epoch 27/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1081 - accuracy: 0.9910 - val_loss: 0.1367 - val_accuracy: 0.9801\n","Epoch 28/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1051 - accuracy: 0.9916 - val_loss: 0.1358 - val_accuracy: 0.9797\n","Epoch 29/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1035 - accuracy: 0.9916 - val_loss: 0.1339 - val_accuracy: 0.9783\n","Epoch 30/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1011 - accuracy: 0.9919 - val_loss: 0.1314 - val_accuracy: 0.9785\n","Epoch 31/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.1003 - accuracy: 0.9920 - val_loss: 0.1253 - val_accuracy: 0.9803\n","Epoch 32/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0975 - accuracy: 0.9928 - val_loss: 0.1256 - val_accuracy: 0.9800\n","Epoch 33/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0969 - accuracy: 0.9927 - val_loss: 0.1203 - val_accuracy: 0.9831\n","Epoch 34/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0958 - accuracy: 0.9925 - val_loss: 0.1228 - val_accuracy: 0.9813\n","Epoch 35/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0948 - accuracy: 0.9929 - val_loss: 0.1212 - val_accuracy: 0.9813\n","Epoch 36/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0935 - accuracy: 0.9930 - val_loss: 0.1223 - val_accuracy: 0.9805\n","Epoch 37/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0931 - accuracy: 0.9927 - val_loss: 0.1193 - val_accuracy: 0.9814\n","Epoch 38/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0924 - accuracy: 0.9931 - val_loss: 0.1231 - val_accuracy: 0.9804\n","Epoch 39/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0905 - accuracy: 0.9936 - val_loss: 0.1243 - val_accuracy: 0.9797\n","Epoch 40/40\n","469/469 [==============================] - 3s 6ms/step - loss: 0.0901 - accuracy: 0.9936 - val_loss: 0.1258 - val_accuracy: 0.9781\n","Regularization factor: 0.001, Replicate: 3, Test loss: 0.1258085072040558, Test accuracy 0.9781000018119812\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 640x480 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["## Our model ##\n","\n","epochs = 40\n","reg_factors = [0.000001, 0.00001, 0.0001, 0.0005, 0.001]\n","\n","# Dictionary to store accuracy scores for each regularization factor\n","reg_factor_dict = {}\n","\n","# Loop over each regularization factor and train the model three times for each factor\n","for reg_factor in reg_factors:\n","    score_list = []\n","    for i in range(3):\n","        # Define the model architecture\n","        model = Sequential()\n","        model.add(Flatten())\n","        model.add(Dense(500, activation='relu', kernel_regularizer=regularizers.l2(reg_factor)))\n","        model.add(Dense(300, activation='relu', kernel_regularizer=regularizers.l2(reg_factor)))\n","        model.add(Dense(num_classes, activation='softmax'))\n","\n","        # Compile the model\n","        model.compile(loss=keras.losses.categorical_crossentropy,\n","                      optimizer=tensorflow.keras.optimizers.SGD(learning_rate=0.1),\n","                      metrics=['accuracy'])\n","\n","        # Train the model\n","        fit_info = model.fit(x_train, y_train,\n","                             batch_size=batch_size,\n","                             epochs=epochs,\n","                             verbose=1,\n","                             validation_data=(x_test, y_test))\n","        \n","        # Evaluate the model on the test set and store the accuracy score\n","        score = model.evaluate(x_test, y_test, verbose=0)\n","        score_list.append(score[1])\n","        print('Regularization factor: {}, Replicate: {}, Test loss: {}, Test accuracy {}'.format(\n","              reg_factor, i+1, score[0], score[1]))\n","    \n","    # Store the accuracy scores for the current regularization factor in the dictionary\n","    reg_factor_dict[reg_factor] = score_list\n","\n","# Extract data and organize into lists\n","data = list(reg_factor_dict.values())\n","reg = [str(k) for k in reg_factor_dict.keys()]\n","\n","\n","# Calculate standard deviation and mean for each group\n","stds = [np.std(d) for d in data]\n","means = [np.mean(d) for d in data]\n","\n","# Create boxplot with median, and red errorbar with mean and standard deviation \n","fig, ax = plt.subplots()\n","ax.boxplot(data, labels=reg)\n","ax.set_xlabel('Regularization factor')\n","ax.set_ylabel('Val accuracy')\n","ax.errorbar(np.arange(1, len(reg) + 1), means, yerr=stds, fmt='o', color='red')\n","ax.set_title('Boxplot with Mean and Standard Deviation')\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"cell_id":"6060cd729f0a4601950657e0a787a73b","deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["np.arange() is a function in the NumPy library of Python that is used to create an array of evenly spaced values within a specified interval. np.arange(1, len(labels) + 1) creates an array of indices ranging from 1 to the number of labels in the dictionary plus 1. This array of indices is used as the x-coordinates for the error bars, to ensure that they are centered above each boxplot. yerr=stds sets the height of the error bars to the standard deviation of the data for each label in the dictionary."]},{"cell_type":"markdown","metadata":{"cell_id":"32c693d939fa4ba19eb47af6f21f1077","deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":259},"deepnote_cell_type":"text-cell-p","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":9,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":["Answers: "]},{"cell_type":"markdown","metadata":{"cell_id":"188c6253-3f94-44fb-94a5-9338c9d24a41","deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":262},"deepnote_cell_type":"text-cell-p","formattedRanges":[{"fromCodePoint":1,"marks":{"bold":true},"toCodePoint":5,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":[" 2.1)   There are 3 layers in total."]},{"cell_type":"markdown","metadata":{"cell_id":"be888f98-8044-45ac-b8b8-451c2a5a8240","deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":265},"deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["The first \"layer\" is flattening the 28x28 matrices to a 1D vector with 784 elements. This is the input layer and doesn't compute anything, thus we don't count it as a layer."]},{"cell_type":"markdown","metadata":{"cell_id":"66028923-9962-4976-ac8e-421319b4e3d1","deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":268},"deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["The remaining layers uses activation functions which decides whether you should activate a neuron or not, and if activated the function decides the output of that neuron."]},{"cell_type":"markdown","metadata":{"cell_id":"3365c9f9-86bc-4199-b110-0da6995f1d99","deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":271},"deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["The first and second \"real\" layers are hidden layers with 64 neurons each using ReLU activation function. ReLu is a common activation function for the hidden layers because it often result in better performance and training. It outputs the maximum of 0 and the input value. One reason why this trains the models faster is because compared to other activation functions it does not activate all neurons in the layer, only the ones with a positive input. "]},{"cell_type":"markdown","metadata":{"cell_id":"ef074e98-b5c1-4cb5-8a4b-298c0b78bb05","deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":274},"deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["The third layer has 10 neurons, one for each possible output number (0-9), and uses Softmax activation, transforming input numbers into probabilities of possible outcomes and is generally used in the output layer of a NN. This allows the network to classify based on probability of being one of the classes given. "]},{"cell_type":"markdown","metadata":{"cell_id":"eeb94cdb17164be3b20be1b03c78a844","deepnote_app_coordinates":{"h":3,"w":12,"x":0,"y":277},"deepnote_cell_type":"markdown","tags":[]},"source":["![Picture title](image-20230302-173828.png)"]},{"cell_type":"markdown","metadata":{"cell_id":"3d0538fc0c02402d9d41d5440b2041ce","deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":281},"deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":[]},{"cell_type":"markdown","metadata":{"cell_id":"6b65a184-e173-49e7-b270-701f2d159b09","deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":284},"deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["We have calculated the total amount of parameters in the following way:"]},{"cell_type":"markdown","metadata":{"cell_id":"15300cb6-c05f-4e8a-b5ff-7f959a1c59da","deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":287},"deepnote_cell_type":"text-cell-p","formattedRanges":[{"fromCodePoint":583,"marks":{"bold":true},"toCodePoint":589,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":["The flatten layer has no trainable parameters.\n","The first dense layer has 64 * (784 + 1) = 50,240 trainable parameters, where 784 is the number of pixels in the input image (28x28) and 1 is for the bias term. There is only 1 bias term in each layer since the bias term is shared across all neurons in that layer. That means the bias term affects the output of the entire layer.\n","The second dense layer has 64 * (64 + 1) = 4,160 trainable parameters.\n","The output dense layer has 10 * (64 + 1) = 650 trainable parameters.\n","Total number of trainable parameters = 50,240 + 4,160 + 650 = 54,050."]},{"cell_type":"markdown","metadata":{"cell_id":"10fadf3b-e368-4fd3-b719-07645ba080ce","deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":290},"deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["The input layer has dimension 784 because the input images are grayscale with dimensions 28x28. The output layer has dimension 10 because there are 10 possible numbers to output."]},{"cell_type":"markdown","metadata":{"cell_id":"d14bc750-0e1e-4270-b7d7-6c43fd70e123","deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":293},"deepnote_cell_type":"text-cell-p","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":5,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":[" 2.2) The used loss function is cross-entropy, as specified in the code. An advantage of cross-entropy is that it can handle multiple classes well, making it suitable for this task where we are trying to classify images into 10 different classes.  "]},{"cell_type":"markdown","metadata":{"cell_id":"ab5dad43-7cd2-4b56-9d2a-54dced35a821","deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":296},"deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":["The mathematical formula for cross-entropy is shown below. When we multiply the true label with the predicted probability, we are only keeping the element of the true class in the 1D vector. This is because the true label vector is 1 for the true class and 0 for all other classes, for example [0,0,0,0,1,0,0]. All the other labels are multiplied with 0 and lose their value. That means this calculation takes the log of the predicted probability for the true class and multiplying it by -1."]},{"cell_type":"markdown","metadata":{"cell_id":"1010db7a-b0e2-4839-8577-204b97e1ac75","deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":299},"deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":[]},{"cell_type":"markdown","metadata":{"cell_id":"b52cdabfaaab46f9b2c357ef7d23626b","deepnote_app_coordinates":{"h":3,"w":12,"x":0,"y":302},"deepnote_cell_type":"markdown","tags":[]},"source":["![Picture title](image-20230302-194014.png)"]},{"cell_type":"markdown","metadata":{"cell_id":"0361642c-24f3-4efd-b18d-b601fe3ff27c","deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":306},"deepnote_cell_type":"text-cell-p","formattedRanges":[{"fromCodePoint":1,"marks":{"bold":true},"toCodePoint":5,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":[" 2.3) See prints"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"c00d4ad2-e0a4-4342-8052-71824027ff8a","deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":309},"deepnote_cell_type":"text-cell-p","formattedRanges":[{"fromCodePoint":1,"marks":{"bold":true},"toCodePoint":6,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":[" 2.4) Our best accuracy was 0.983 with regularization factor 0.0005.\n","\n","To get a background we will explain L2 regularization, also known as weight decay. It is a technique used to prevent overfitting in machine learning models by adding a penalty to the loss function. The penalty encourages the network to learn smaller weight values, and thus improving its generalization ability. A higher regularization factor will make the weights smaller. On the other hand, a lower regularization factor will result in bigger weights. That means that choosing a too-high value can lead to underfitting, while a too-low value can lead to overfitting.\n","\n","We did not reach the same accuracy rate as Hinton. We were 0.002 accuracy away from his results and there might be several explanations to this that was not stated in the paper. Mainly the batch size was not given by Hinton. Batch size refers to the number of training examples used in one iteration of the algorithm during the training of a neural network, and thus is of great importance. A larger batch size can result in more accurate gradient estimates and faster convergence, but it requires more memory to store the intermediate activations and gradients, and may lead to slower training times. A smaller batch size can lead to noisy gradient estimates and slower convergence, but it requires less memory and may lead to faster training times. \n","\n","We used a batch size of 128 and theoretically a higher batch size would have given a better result, closer to what Hinton got. \n","\n"]},{"cell_type":"markdown","metadata":{"cell_id":"4436cf6ffd0847da9552c9862aaeac77","deepnote_app_coordinates":{"h":13,"w":12,"x":0,"y":325},"deepnote_cell_type":"markdown","tags":[]},"source":["## 3. Convolutional layers \n"," \n","3.1. Design a model that makes use of at least one convolutional layer – how performant a \n","model can you get? -- According to the MNIST database it should be possible reach to \n","99%  accuracy  on  the  validation  data.  If  you  choose  to  use  any  layers  apart  from  the \n","convolutional layers and layers that you used in previous questions, you must describe \n","what  they  do.  If  you  do not  reach 99%  accuracy,  report  your  best  performance,  and \n","explain your attempts and thought process. \n"," \n","3.2. Discuss the differences and potential benefits of using convolutional layers over fully \n","connected ones for the application?  \n"]},{"cell_type":"code","execution_count":4,"metadata":{"cell_id":"002ef6a4582d43a7adecd4606fd0a312","deepnote_app_coordinates":{"h":45,"w":12,"x":0,"y":339},"deepnote_cell_type":"code","deepnote_to_be_reexecuted":false,"execution_millis":89460,"execution_start":1678269841294,"source_hash":"48885d25","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/40\n","469/469 [==============================] - 18s 37ms/step - loss: 0.3793 - accuracy: 0.8838 - val_loss: 0.0908 - val_accuracy: 0.9693\n","Epoch 2/40\n","469/469 [==============================] - 17s 37ms/step - loss: 0.0862 - accuracy: 0.9733 - val_loss: 0.0751 - val_accuracy: 0.9756\n","Epoch 3/40\n","469/469 [==============================] - 17s 36ms/step - loss: 0.0626 - accuracy: 0.9804 - val_loss: 0.0414 - val_accuracy: 0.9873\n","Epoch 4/40\n","469/469 [==============================] - 17s 36ms/step - loss: 0.0521 - accuracy: 0.9837 - val_loss: 0.0435 - val_accuracy: 0.9853\n","Epoch 5/40\n","469/469 [==============================] - 17s 36ms/step - loss: 0.0414 - accuracy: 0.9870 - val_loss: 0.0384 - val_accuracy: 0.9868\n","Epoch 6/40\n","469/469 [==============================] - 17s 36ms/step - loss: 0.0360 - accuracy: 0.9886 - val_loss: 0.0446 - val_accuracy: 0.9852\n","Epoch 7/40\n","469/469 [==============================] - 17s 36ms/step - loss: 0.0310 - accuracy: 0.9898 - val_loss: 0.0345 - val_accuracy: 0.9893\n","Epoch 8/40\n","469/469 [==============================] - 17s 37ms/step - loss: 0.0264 - accuracy: 0.9916 - val_loss: 0.0339 - val_accuracy: 0.9886\n","Epoch 9/40\n","469/469 [==============================] - 17s 37ms/step - loss: 0.0234 - accuracy: 0.9920 - val_loss: 0.0322 - val_accuracy: 0.9890\n","Epoch 10/40\n","469/469 [==============================] - 17s 37ms/step - loss: 0.0216 - accuracy: 0.9931 - val_loss: 0.0318 - val_accuracy: 0.9893\n","Epoch 11/40\n","469/469 [==============================] - 17s 36ms/step - loss: 0.0178 - accuracy: 0.9941 - val_loss: 0.0348 - val_accuracy: 0.9892\n","Epoch 12/40\n","469/469 [==============================] - 17s 37ms/step - loss: 0.0165 - accuracy: 0.9947 - val_loss: 0.0326 - val_accuracy: 0.9905\n","Epoch 13/40\n","469/469 [==============================] - 17s 37ms/step - loss: 0.0132 - accuracy: 0.9957 - val_loss: 0.0362 - val_accuracy: 0.9881\n","Epoch 14/40\n","469/469 [==============================] - 18s 38ms/step - loss: 0.0116 - accuracy: 0.9964 - val_loss: 0.0291 - val_accuracy: 0.9910\n","Epoch 15/40\n","469/469 [==============================] - 17s 37ms/step - loss: 0.0107 - accuracy: 0.9967 - val_loss: 0.0330 - val_accuracy: 0.9902\n","Epoch 16/40\n","469/469 [==============================] - 17s 37ms/step - loss: 0.0093 - accuracy: 0.9973 - val_loss: 0.0402 - val_accuracy: 0.9885\n","Epoch 17/40\n","469/469 [==============================] - 17s 36ms/step - loss: 0.0090 - accuracy: 0.9970 - val_loss: 0.0374 - val_accuracy: 0.9892\n","Epoch 18/40\n","469/469 [==============================] - 18s 38ms/step - loss: 0.0074 - accuracy: 0.9977 - val_loss: 0.0306 - val_accuracy: 0.9913\n","Epoch 19/40\n","469/469 [==============================] - 18s 38ms/step - loss: 0.0059 - accuracy: 0.9982 - val_loss: 0.0316 - val_accuracy: 0.9905\n","Epoch 20/40\n","469/469 [==============================] - 18s 38ms/step - loss: 0.0048 - accuracy: 0.9987 - val_loss: 0.0315 - val_accuracy: 0.9911\n","Epoch 21/40\n","469/469 [==============================] - 17s 37ms/step - loss: 0.0053 - accuracy: 0.9985 - val_loss: 0.0352 - val_accuracy: 0.9900\n","Epoch 22/40\n","469/469 [==============================] - 17s 37ms/step - loss: 0.0037 - accuracy: 0.9990 - val_loss: 0.0372 - val_accuracy: 0.9909\n","Epoch 23/40\n","469/469 [==============================] - 17s 37ms/step - loss: 0.0032 - accuracy: 0.9991 - val_loss: 0.0363 - val_accuracy: 0.9900\n","Epoch 24/40\n","469/469 [==============================] - 17s 37ms/step - loss: 0.0033 - accuracy: 0.9991 - val_loss: 0.0411 - val_accuracy: 0.9897\n","Epoch 25/40\n","469/469 [==============================] - 17s 37ms/step - loss: 0.0024 - accuracy: 0.9994 - val_loss: 0.0410 - val_accuracy: 0.9900\n","Epoch 26/40\n","469/469 [==============================] - 17s 37ms/step - loss: 0.0018 - accuracy: 0.9995 - val_loss: 0.0384 - val_accuracy: 0.9903\n","Epoch 27/40\n","469/469 [==============================] - 17s 37ms/step - loss: 0.0011 - accuracy: 0.9999 - val_loss: 0.0354 - val_accuracy: 0.9912\n","Epoch 28/40\n","469/469 [==============================] - 18s 38ms/step - loss: 9.6468e-04 - accuracy: 0.9998 - val_loss: 0.0366 - val_accuracy: 0.9907\n","Epoch 29/40\n","469/469 [==============================] - 17s 37ms/step - loss: 8.5256e-04 - accuracy: 0.9998 - val_loss: 0.0394 - val_accuracy: 0.9903\n","Epoch 30/40\n","469/469 [==============================] - 17s 37ms/step - loss: 6.6935e-04 - accuracy: 0.9999 - val_loss: 0.0408 - val_accuracy: 0.9907\n","Epoch 31/40\n","469/469 [==============================] - 17s 37ms/step - loss: 6.9391e-04 - accuracy: 0.9998 - val_loss: 0.0390 - val_accuracy: 0.9906\n","Epoch 32/40\n","469/469 [==============================] - 17s 37ms/step - loss: 3.8534e-04 - accuracy: 1.0000 - val_loss: 0.0379 - val_accuracy: 0.9912\n","Epoch 33/40\n","469/469 [==============================] - 18s 39ms/step - loss: 3.8921e-04 - accuracy: 0.9999 - val_loss: 0.0390 - val_accuracy: 0.9911\n","Epoch 34/40\n","469/469 [==============================] - 18s 38ms/step - loss: 3.0803e-04 - accuracy: 1.0000 - val_loss: 0.0385 - val_accuracy: 0.9912\n","Epoch 35/40\n","469/469 [==============================] - 18s 38ms/step - loss: 2.4301e-04 - accuracy: 1.0000 - val_loss: 0.0393 - val_accuracy: 0.9912\n","Epoch 36/40\n","469/469 [==============================] - 17s 37ms/step - loss: 2.9222e-04 - accuracy: 1.0000 - val_loss: 0.0396 - val_accuracy: 0.9907\n","Epoch 37/40\n","469/469 [==============================] - 17s 36ms/step - loss: 2.3511e-04 - accuracy: 1.0000 - val_loss: 0.0412 - val_accuracy: 0.9905\n","Epoch 38/40\n","469/469 [==============================] - 17s 37ms/step - loss: 2.7118e-04 - accuracy: 1.0000 - val_loss: 0.0404 - val_accuracy: 0.9905\n","Epoch 39/40\n","469/469 [==============================] - 18s 38ms/step - loss: 2.0440e-04 - accuracy: 1.0000 - val_loss: 0.0409 - val_accuracy: 0.9908\n","Epoch 40/40\n","469/469 [==============================] - 18s 37ms/step - loss: 1.8563e-04 - accuracy: 1.0000 - val_loss: 0.0407 - val_accuracy: 0.9911\n","Test loss: 0.040662821382284164, Test accuracy 0.991100013256073\n"]}],"source":["epochs = 40\n","\n","model = Sequential()\n","model.add(Conv2D(32, kernel_size=(3, 3), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n","model.add(Conv2D(64, kernel_size=(5, 5), activation='relu'))\n","model.add(MaxPooling2D(pool_size=(2,2), strides=2))\n","model.add(Flatten())\n","model.add(Dense(64, activation = 'relu'))\n","model.add(Dense(64, activation = 'relu'))\n","model.add(Dense(num_classes, activation='softmax'))\n","\n","\n","model.compile(loss=keras.losses.categorical_crossentropy,\n","               optimizer=tensorflow.keras.optimizers.SGD(learning_rate = 0.1),\n","        metrics=['accuracy'],)\n","\n","fit_info = model.fit(x_train, y_train,\n","           batch_size=batch_size,\n","           epochs=epochs,\n","           verbose=1,\n","           validation_data=(x_test, y_test))\n","score = model.evaluate(x_test, y_test, verbose=0)\n","print('Test loss: {}, Test accuracy {}'.format(score[0], score[1]))"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"3de6434b2f144132b9e28e4537ebebc6","deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":385},"deepnote_cell_type":"text-cell-p","formattedRanges":[{"fromCodePoint":0,"marks":{"bold":true},"toCodePoint":8,"type":"marks"}],"is_collapsed":false,"tags":[]},"source":["Answer:"]},{"attachments":{},"cell_type":"markdown","metadata":{"cell_id":"f3ad7563-a9d8-42d0-a6c3-740a17f72033","deepnote_app_coordinates":{"h":2,"w":8,"x":0,"y":388},"deepnote_cell_type":"text-cell-p","formattedRanges":[],"is_collapsed":false,"tags":[]},"source":[" 3.1) We have used 4 convolutional layer. Firstly we added a Conv2D layer with 32 filters, a kernel size of 3x3, and ReLU activation. This layer applies 32 filters to the input image, each filter extracting a feature. Then we added a MaxPooling2D layer with a pool size of 2x2 and stride of 2. This layer downsamples the feature maps generated by the previous convolutional layer by selecting the maximum value in each 2x2 pool, and the stride= 2 means that it looks at every other pixel. Then we added another Conv2D layer with 64 filters, a kernel size of 5x5, and ReLU activation. This layer applies 64 filters to the feature maps generated by the previous layer and we choose 5x5 and 64 filters because we wanted this layer to extract more features. Lastly we added another MaxPooling2D layer with the same parameters as before.\n","\n","When first tried with only 2 layers, this resulted in a lower accuracy then we wanted. So we added a second 2D convolutional layer with 3x3 kernel size and a MaxPooling2D layer afterwards, the accuracy was still too low. We then tried to expand the kernel size of the Conv2D layer to sizer 5x5. This resulted in a accuracy of at least 99% which was what we were looking for. \n","\n","3.2) A convolutional layer is a set of filters applied to the input image, and are designed to exploit the spatial correlation between adjacent pixels in an image. The filters are merged with the input image, producing a set of feature maps. This is particularly important for image classification tasks where the orientation and location of the object of interest can vary. These feature maps are then passed through a non-linear activation function, such as ReLU, to introduce non-linearity into the model. The filters can help the model focus on certain parts of the image, making it better suited for certain tasks, such as detecting edges, shapes, and textures. \n","\n","Fully connected layers, on the other hand, consider the input image as a one-dimensional vector, disregarding its spatial arrangement. By linking each neuron in the present layer to all neurons in the following layer, they require a significant number of parameters. This methodology is not ideal for image recognition, as it cannot preserve the spatial pattern of the image and would necessitate a large number of parameters, and computing power.\n","\n","The benefits are therefore that the convolutional layers can capture spatial information of an image and is more resource efficient.\n","\n"]},{"cell_type":"markdown","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown","tags":[]},"source":["<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=ed0c436b-4e08-4c3b-a423-80f934892d31' target=\"_blank\">\n","<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n","Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Assignment_7_NN.ipynb","provenance":[]},"deepnote":{},"deepnote_app_layout":"article","deepnote_execution_queue":[],"deepnote_notebook_id":"2a5d613444a54afab843e2f9addee4bd","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":0}
